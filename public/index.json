[{"content":"As a software engineer, I have written a lot of lines of code, and taking a hindsight back to the very first dayd of my career, I have written a log of bad code.\nRecently, I published a new version of my react-here-maps library which saw a sprinkle of my improved experience\n","permalink":"https://limistah.dev/posts/imparative-declarative-coding/","summary":"As a software engineer, I have written a lot of lines of code, and taking a hindsight back to the very first dayd of my career, I have written a log of bad code.\nRecently, I published a new version of my react-here-maps library which saw a sprinkle of my improved experience","title":"Imparative and Declarative coding?"},{"content":"Byte masking is a deep CS concept reserved for the nerds. Here we will attempt to dissect the topic and provide a relatable experience for everyone.\nWelcome\u0026hellip;\nMasking and its Relation to CS Masking is a process of concealing information. Take for example having a string \u0026quot;A\u0026quot; but revealing \u0026ldquo;X\u0026rdquo; to others such that only those with the information on how to get the hidden value can retrieve it.\nA byte is a group of bits(1 and 0) usually eight in number. Such that 00000000 becomes a byte but the individual zeros are known as bits.\nIn CS bitmasking is a process of manipulating the bits of computational data. For example, converting 00000000 to 01010101. In CS, it mostly denotes what bit you want to keep, and what bit you wish to clear. in our case, we have divided the byte into pairs and cleared the first bit 00|00|00|00 =\u0026gt; 01|01|01|01.\nMasking Operators Masking bits is done by the use of masking operators, taking from the byte operators:\nThe Bitwise OR | (a single pipe character) The Bitwise AND \u0026amp; (a single ampersand character) The Bitwise XOR ^ (a single caret character) The Left Shift \u0026laquo; (a double less than character) The Right Shift \u0026raquo; (a double greater than character) Masking operations examples OR The rule for bitwise OR is as follows:\nIf at least one of the corresponding bits is 1, the result bit is set to 1. If both corresponding bits are 0, the result bit is set to 0. 10101010 (binary number) | 11001100 (binary number) __________ 11101110 (result) AND Here\u0026rsquo;s how the bitwise AND operator works:\nIf both bits in the operands are 1, the result will be 1. If at least one of the bits is 0, the result will be 0. 10101010 \u0026amp; 11001100 ----------- 10001000 XOR Here\u0026rsquo;s how the XOR operator works:\nIf the bits in the operands are different (one is 0 and the other is 1), the result will be 1. If the bits in the operands are the same (both 0 or both 1), the result will be 0. 10101010 ^ 11001100 ----------- 01100110 Left Shift Here\u0026rsquo;s how the left-shift operator works:\nEach bit in the binary representation of the operand is shifted to the left by a certain number of positions. Zeros are filled in from the right, and the bits that are shifted beyond the leftmost position are discarded. The result is obtained by multiplying the original value by 2 raised to the power of the specified shift count. Original Value: 00101010 (42 in decimal) Left Shift by 2: 10101000 (168 in decimal) Each bit in the original binary value 00101010 is shifted to the left by 2 positions. Zeros are filled in from the right, and the result is 10101000, which is equivalent to 168 in decimal.\nRight Shift Here\u0026rsquo;s how the right shift operator works:\nEach bit in the binary representation of the operand is shifted to the right by a certain number of positions. Depending on the type of right shift (logical or arithmetic), zeros or the sign bit (the leftmost bit) are filled in from the left. The bits that are shifted beyond the rightmost position are discarded. The result is obtained by dividing the original value by 2 raising to the power of the specified shift count (for logical right shift) or retaining the sign bit (for arithmetic right shift). Original Value: 10101010 (170 in decimal) Right Shift by 2: 00101010 (42 in decimal) There are two types of right shift:\nLogical Right Shift (\u0026gt;\u0026gt;\u0026gt;): Zeros are filled in from the left, and the sign bit is always 0. This type of shift is commonly used in programming for unsigned integers. Arithmetic Right Shift (\u0026gt;\u0026gt;): Zeros or the sign bit are filled in from the left, depending on the sign bit of the original value. This type of shift is used in signed integer arithmetic to preserve the sign. How masking helps our daily lives When you shouldn\u0026rsquo;t consider masking Adios\n","permalink":"https://limistah.dev/posts/byte-masking/","summary":"Byte masking is a deep CS concept reserved for the nerds. Here we will attempt to dissect the topic and provide a relatable experience for everyone.\nWelcome\u0026hellip;\nMasking and its Relation to CS Masking is a process of concealing information. Take for example having a string \u0026quot;A\u0026quot; but revealing \u0026ldquo;X\u0026rdquo; to others such that only those with the information on how to get the hidden value can retrieve it.\nA byte is a group of bits(1 and 0) usually eight in number.","title":"Byte Masking the ins and out"},{"content":"In Ruby, the if statement looks like this\nval = 1 if val == 1 p \u0026#34;Equality Checked!\u0026#34; end And for if else\nval = 2 if val == 1 p \u0026#34;Equality Checked!\u0026#34; else p \u0026#34;Equality Unchecked!\u0026#34; end And for if, else if, else\nval = 2 if val == 1 p \u0026#34;Equality Checked!\u0026#34; elsif val == 2 p \u0026#34;Equality Middle Checked!\u0026#34; else p \u0026#34;Equality Unchecked!\u0026#34; end Also, remember that everything in ruby returns a value, so your if statement can return a value that could be stored in another variable.\nval = 2 # store the returned value from the if statement ret_val = if val == 1 p \u0026#34;Equality Checked!\u0026#34; else p \u0026#34;Equality Unchecked!\u0026#34; end p ret_val # =\u0026gt; \u0026#34;Equality Unchecked!\u0026#34; Dhanyavaad! 🙇\n","permalink":"https://limistah.dev/posts/ruby-if-conditions/","summary":"In Ruby, the if statement looks like this\nval = 1 if val == 1 p \u0026#34;Equality Checked!\u0026#34; end And for if else\nval = 2 if val == 1 p \u0026#34;Equality Checked!\u0026#34; else p \u0026#34;Equality Unchecked!\u0026#34; end And for if, else if, else\nval = 2 if val == 1 p \u0026#34;Equality Checked!\u0026#34; elsif val == 2 p \u0026#34;Equality Middle Checked!\u0026#34; else p \u0026#34;Equality Unchecked!\u0026#34; end Also, remember that everything in ruby returns a value, so your if statement can return a value that could be stored in another variable.","title":"Ruby - if statement?"},{"content":"Part of the operators we get introduced to when learning to program is Bitwise Operators, examples are:\nThe Bitwise OR | (a single pipe character) The Bitwise AND \u0026amp; (a single ampersand character) The Bitwise XOR ^ (a single caret character) Each of these has its usage, a refresher can be demonstrated considering these two variables foo=1 and bar=0\nFor the bitwise OR(|) operator const foo = 1, bar = 0 console.log(foo | bar) -\u0026gt; 1 For the bitwise AND(\u0026amp;) operator const foo = 1, bar = 0 console.log(foo \u0026amp; bar) -\u0026gt; 0 For the bitwise XOR(^) operator const foo = 1, bar = 0 console.log(foo \u0026amp; bar) -\u0026gt; 0 This seems pretty basic until you understand it is not.\nWhat the heck is byte masking? Byte masking is a process of flipping the smallest unit of computing value called bits.\nSome CS Background Computer as you interact with it is basically 1s and 0s on the lowest level. This text you are reading is represented as a bunch of 1s and 0s inside the computer. These values represent the state of a very tiny piece of a device called a transistor.\nTransistors as electronic devices are very unique and useful, they behave like light bulbs (i.e. they can be turned on and off), what makes them very useful to us today is that they can maintain their state(either on or off) over a very long time. Each of these states is represented as either on(0) or off (1).\nBits to Bytes After the advancement of transistors, the next phase is on how to make them useful for everyday usage(what technology is meant for), which is how to make transistors as a storage devices. But we can\u0026rsquo;t store the Alphabet as on and off or 1 and 0, the byte was invented.\nA byte is a representation of a group of bits. Eight(8) bits would produce One(1) byte. Storing this on transistors means that transistors have to be grouped, and each group would have an identity. For example, if we have 32 transistors, that is 32 bits, and can be further converted to 32 / 8 = 4bytes, another example is if we have 32000 bits we would have 4000bytes. The 4000bytes can be simplified by dividing by 1000 to create a kilo version, hence 4000byes/1000=4Kbytes\nBytes to ASCII code With computers stuck at 1s and 0s, it makes more sense to stick to just the base 2 numbering system for computer arithmetic operations. For example, 1 + 1 in base 2 equals 0 (add both numbers, divide by 2 then take note of the remainder after the division), and 1+0 in base 2 equals 1 (subtract both numbers, then divide by 2, then take note of the remainder after the division).\nWith these genius concepts, invention went further by assigning numbers to every number, character, and symbol ever known to man. This assignment is called ASCII character code. A(capital alphabet) is a different character from a(smaller alphabet) and both have unique ASCII numbers. For A the ASCII code is 065 and ais 097.\nRepresenting characters Representing a character on a transistor(in bits) becomes easier, convert the code from base 10 to 2. For A the base 10 value is 065, while the base 2 value is 01000001 and for the small a with a base 10 value of 097 the base 2 value is 01100001. Numbers written in base 10 are called Decimal Numbers, while numbers written in base 2 are called Binary numbers.\nRepresenting words Since we can store numbers, characters, and symbols, we should be able to store words. Words are a group of letters and somehow letters, and alphabets, it is safe to use string, so a string of numbers, alphabets, and symbols.\nThis can be easily done by taking a consecutive byte(recall that this means 8 units of bits) until all the characters in the string are represented. For example, to represent hello we would take the first byte and fill it with 01101000 (104 in ASCII) for h, then 01100101 (101 in ASCII) for e, then 01101100 (108 in ASCII) for l, then 01001100 (076 in ASCII) for l, and 01101111 (111) for o.\nGrouping this together it forms a string of 0s and 1s like this\n0110100001100101011011000110110001101111.\nWhat is byte masking? Byte masking is data manipulation at the bit level. What is happening is based on the kind of byte masking operation, we are instructing the transistor to switch to another state or maintain their state. For example, the binary code for the number 1 is 1, and for 2 is 10, and a bitwise OR operator on both numbers would give 3.\nconst foo = 1 // 01 in binary const bar = 2 // 10 in binary foo | bar // =\u0026gt; 3 , 11 in binary What is happening here is in the way the bitwise OR operator works. It adds up the binary numbers, divides the result by 2, and records the remainder. Here if we do a right-to-left addition, 1+0 would give 1 divided by 2 gives zero but left with 1, and to the right we have the same operation we would end up with 1 as well. But, 11 is a binary representation of 3 in decimal numbers.\n0 1 | 1 0 ------ 1 1 # Taking the top right and the bottom right 1 + 0 = 1 1 / 2 = 0 remainder 1, take the remainder as the answer # Taking the top left and the bottom left 0 + 1 = 1 1 / 2 = 0 remainder 1, take the remainder as the answer #------ 11 in base 2 is 3 in base 10 1 | 2 returns 3 Be aware that 1 is not stored as 1 in computers but as 00000001 same for 2, it is rather stored as 00000010 for consistency against larger values.\nMasking Operators To better understand byte masking operations, take a look at what each masking operator would do for you.\nThe Bitwise OR operator As shown earlier, this operator returns the result of adding two bytes together. In the case of 1 and 2, we added 00000001 and 00000010 together to result into this 00000011.\nIf the addition seems more arithmetic here is a better way to understand this:\n// OR Operator 00000001 00000010 = 00000011 As long as there is 1 in any of the values we are comparing, the result must return 1.\nThe Bitwise AND Operator This is the reverse of the bitwise OR operator. Instead of taking the remainder, we take the result of the division by 2.\n# 01 \u0026amp; 10 0 1 \u0026amp; 1 0 ------- 0 0 # Taking the top right and the bottom right 1 + 0 = 1 1 / 2 = 0 remainder 1, take the result of the division as the answer # Taking the top left and the bottom left 0 + 1 = 1 1 / 2 = 0 remainder 1, take the result of the division as the answer ----- 00 in base 2 is 0 in base 10 1 \u0026amp; 2 returns 0 A better way to understand this without the math\n# AND Operator +-------------------------------+ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | +---+---+---+---+---+---+---+---+ \u0026amp; | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | +-------------------------------+ = | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | +-------------------------------+ As long as there is a 0 in any of the columns we are comparing, the result must return 0.\nThe Bitwise XOR Operator For the XOR operator, it checks if the remainder after the division of the two values is equal to 0, if it is, then the result is the same as the division (0, mostly for 0 + 1, 1 + 0 operations) , if the remainder after the division is 1 then the result is the remainder (0, mostly for 1 + 1 operations).\n# XOR Operator 101 | 110 1 0 1 | 1 1 0 ------- 0 1 1 Taking the top right and the bottom right 1 + 0 = 1 1 / 2 = 0 remainder 1, take the remainder as the answer since the result is 0 Taking the top middle and the bottom middle 0 + 1 = 1 1 / 2 = 0 remainder 1, take the remainder as the answer since the result is 0 Taking the top left and the bottom left 1 + 1 = 2 2 / 2 = 1 remainder 0, take the result of the division as the answer since the remainder is 0 ----- 11 in base 2 is 3 in base 10 1 | 2 returns 3 Without the arithmetic:\n# 10000101 ^ 00000110 +-------------------------------+ | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | +---+---+---+---+---+---+---+---+ ^ | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | +-------------------------------+ = | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | +-------------------------------+ The Bitwise NOT operator This operator basically negates the value of each bit. If a bit is 0 it becomes 1 and if it is 1 it basically becomes 0. For example:\n# NOT Operator ~ 101 ----- = 010 Also,\n# NOT Operator ~ 010 ----- = 101 Other operators There are still two more operators, the Left Shift Operator and the Right Shift Operator. Both try to inverse(change to the other value)\nVoila!\n","permalink":"https://limistah.dev/posts/byte-operators/","summary":"Part of the operators we get introduced to when learning to program is Bitwise Operators, examples are:\nThe Bitwise OR | (a single pipe character) The Bitwise AND \u0026amp; (a single ampersand character) The Bitwise XOR ^ (a single caret character) Each of these has its usage, a refresher can be demonstrated considering these two variables foo=1 and bar=0\nFor the bitwise OR(|) operator const foo = 1, bar = 0 console.","title":"What is Byte Masking and how useful is it?"},{"content":"Since everything is an object in Ruby having a functionality that can duplicate objects is not a bad idea.\nRuby ships with two methods for making copies of an object: the dup method and the clone method.\nIn Ruby, all variables hold a reference to an object. In a case where a section of a code modifies an object that is not meant to be modified, it is ideal to make a copy of that object to be used in that section of the code, protecting the integrity of the copied object.\n# initial value of str str = \u0026#34;this is a test string\u0026#34; # accepts a string def modifyAnyString (strVar) # string is replaced with another string changing the value strVar.replace(\u0026#34;this is a modified version of strVar\u0026#34;) end # call the modify string with the str variable modifyAnyString(str) puts str # outputs: this is a modified version of strVar In some languages like C# and Java, this is called passing by value or dereferencing.\nThe dup method Take this multiverse object initialized below:\nmultiverse = Object.new We can pass multiverse to any method that has a write operation, this would affect the object.\ndef getObjectID (obj) obj.object_id # returns the object ID of the passed object end puts getObjectID(multiverse) == multiverse.object_id # true To avoid sending in the exact object by reference use the Object.dup method to create a copy of an object.\nputs getObjectID(multiverse.dup) == multiverse.object_id # false Now, it is safer to pass the duplicated variable to a method. This would protect modifying multiverse objects in the getObjectID method if the method is not meant to do any write operation on the object.\nNote: If an object is frozen, the returned object will remain frozen, even after it has been duplicated.\ndup_multi = multiverse.dup # freeze the dup_multi dup_multi.freeze frz_dup_multi = dup_multi.dup # false original multiverse has not been frozen puts multiverse.frozen? # true The dup_multi was frozen before duplication puts frz_dup_multi.frozen? The copy method Copying an object is almost the same as duplicating an object, the only difference is that, when copying, if the object is frozen, the copied object becomes unfrozen.\ncopy_multi = multiverse.dup # freeze the dup_multi copy_multi.freeze frz_copy_multi = copy_multi.dup # false original multiverse has not been frozen puts multiverse.frozen? # false The copy_multi was frozen before duplication puts frz_copy_multi.frozen? This can be useful to create a copy of the same object that is not restricted either by the function caller or by developers who want to ensure that a method consistently works with unfrozen objects.\nSalut! 🙇\n","permalink":"https://limistah.dev/posts/create-multiple-copies-of-an-object-ruby/","summary":"Since everything is an object in Ruby having a functionality that can duplicate objects is not a bad idea.\nRuby ships with two methods for making copies of an object: the dup method and the clone method.\nIn Ruby, all variables hold a reference to an object. In a case where a section of a code modifies an object that is not meant to be modified, it is ideal to make a copy of that object to be used in that section of the code, protecting the integrity of the copied object.","title":"Creating multiple copies of objects in Ruby"},{"content":"Usually, programming languages have methods for printing out variables. Ruby is not an exception. We will explore the 3 popular methods for printing variables in the Ruby Programming language.\nThe print method The way print(var) works is basically converting its value to a string by calling the to_s method on the object(everything is an object in Ruby) before printing the value and returning nil to its caller.\nnum = 123 print(num) # -\u0026gt; 123 =\u0026gt; nil The print method can be easily used for concatenating strings\nnum = 123 name = \u0026#34;Aleem\u0026#34; print \u0026#34;The name of the boy is \u0026#34; print name print \u0026#34;, and his tag ID is: \u0026#34; print num print \u0026#34;.\u0026#34; # outputs everything on a single line # -\u0026gt; The name of the boy is Aleem, and his tag ID is: 123 Having print as the last operation in a method should be avoided if returning nil is not the desired value\ndef check_print print \u0026#34;This should print without a new line\u0026#34; end val = check_point # -\u0026gt; This should print withouit a new line # Now p val would return nil The puts method puts method is not so different from the print method except for two scenarios:\nputs adds a new line character at the end of the printed value print \u0026#34;hello World\u0026#34; # -\u0026gt; Hello World # -\u0026gt; nil puts \u0026#34;Hello world\u0026#34; # -\u0026gt; Hello world # -\u0026gt; # -\u0026gt; nil puts prints each element in an array on a new line arr = [1,2,3,4,5,6] print arr # -\u0026gt; [1,2,3,4,5,6] # -\u0026gt; nil puts arr # -\u0026gt; 1 # -\u0026gt; 2 # -\u0026gt; 3 # -\u0026gt; 4 # -\u0026gt; 5 # -\u0026gt; 6 # -\u0026gt; nil The p method This method can be seen as a debugging tool. It prints more than just the value of a variable, it can print the memory, the object it belongs to. A good name befitting the p method is the variable inspection method.\np STDERR # -\u0026gt; \u0026lt;IO:\u0026lt;STDERR\u0026gt;\u0026gt; puts STDERR # -\u0026gt; \u0026lt;IO:0x000000013f888e58\u0026gt; Notice that above we have printed the value of STDERR to the console. using the p method, the module that the constant belongs as well as its name is returned, while puts only returns the module and memory address of the STDERR constant.\nShalom 🙇\n","permalink":"https://limistah.dev/posts/when-to-use-puts-print-p-in-ruby/","summary":"Usually, programming languages have methods for printing out variables. Ruby is not an exception. We will explore the 3 popular methods for printing variables in the Ruby Programming language.\nThe print method The way print(var) works is basically converting its value to a string by calling the to_s method on the object(everything is an object in Ruby) before printing the value and returning nil to its caller.\nnum = 123 print(num) # -\u0026gt; 123 =\u0026gt; nil The print method can be easily used for concatenating strings","title":"When to use puts, print, and p in Ruby"},{"content":"The error:\ntar (child): xz: Cannot exec: No such file or directory Is majorly an issue with the xz command not found on the host machine.\nTo verify, run:\nwhereis xz Fix To fix, use the installation command for your Linux distribution:\nsudo apt-get install xz-utils # Debian / Ubuntu sudo yum install xz # RHEL / CentOS sudo zypper in xz # OpenSuSE sudo pacman -S xz # Arch Linux Then untar again with:\ntar -xf path_to_file.tar.xz Ref: S/O\nDhanyavaad! 🙇\n","permalink":"https://limistah.dev/posts/tar-child-xz-cannot-exec/","summary":"The error:\ntar (child): xz: Cannot exec: No such file or directory Is majorly an issue with the xz command not found on the host machine.\nTo verify, run:\nwhereis xz Fix To fix, use the installation command for your Linux distribution:\nsudo apt-get install xz-utils # Debian / Ubuntu sudo yum install xz # RHEL / CentOS sudo zypper in xz # OpenSuSE sudo pacman -S xz # Arch Linux Then untar again with:","title":"tar (child) xz Cannot exec No such file or directory"},{"content":"A onliner for f in *.old; do mv \u0026#34;$f\u0026#34; \u0026#34;${f%.old}.new\u0026#34;; done How??? To change the name of a file on Linux/Unix use the mv command\nmv currentfilename newfilename If there is a file named aleemisiaka.old and want to rename to aleemisiaka.new\nWe can store the filename to a variable with export FILENAME=aleemisiaka\nAnd use the variable name in the mv command\nmv \u0026#34;$FILENAME\u0026#34;$ \u0026#34;$FILENAME.new\u0026#34; This renames the file from aleem-isiaka.old to aleemisiaka.old.new\nTo ensure we end up with aleemisiaka.new and not aleemisiaka.old.new we could use parameter expansion.\necho \u0026quot;${FILENAME%.old}\u0026quot; would give just aleemisiaka without the old.\nNow append the new extension with echo \u0026quot;${FILENAME%.old}.new\u0026quot; which gives aleemisiaka.new\nAwesome\nBack to mv command\nmv \u0026#34;$FILENAME}\u0026#34; \u0026#34;${FILENAME.old}.new\u0026#34; Renames the file from aleemisiaka.old to aleemisiaka.new.\nTo replace all files in a directory use the Bash for loop\nfor f in *.old # Loops through all the files with the .old extension do mv \u0026#34;$f\u0026#34; \u0026#34;${f%.old}.new\u0026#34; # renames from file.old to file.new done # a oneliner for f in *.old; do mv \u0026#34;$f\u0026#34; \u0026#34;${f%.old}.new\u0026#34;; done Au revoir\n","permalink":"https://limistah.dev/posts/change-extension-of-files-in-a-directory/","summary":"A onliner for f in *.old; do mv \u0026#34;$f\u0026#34; \u0026#34;${f%.old}.new\u0026#34;; done How??? To change the name of a file on Linux/Unix use the mv command\nmv currentfilename newfilename If there is a file named aleemisiaka.old and want to rename to aleemisiaka.new\nWe can store the filename to a variable with export FILENAME=aleemisiaka\nAnd use the variable name in the mv command\nmv \u0026#34;$FILENAME\u0026#34;$ \u0026#34;$FILENAME.new\u0026#34; This renames the file from aleem-isiaka.old to aleemisiaka.","title":"Change files extension in a directory"},{"content":"To search through the man pages for some keywords, use the -k option.\nman -k [keyword]\nThis shows a result of the commands, and routines that match the keyword with a one-line description of what they are about.\nman -k passwd shows all the possible entries for passwd in the manual pages.\nThe apropos The man -k [keyword] command is similar to a help utility called apropos which is available both on Unix and Linux. See it as a shortcut.\n","permalink":"https://limistah.dev/posts/apropos-mandb-linux/","summary":"To search through the man pages for some keywords, use the -k option.\nman -k [keyword]\nThis shows a result of the commands, and routines that match the keyword with a one-line description of what they are about.\nman -k passwd shows all the possible entries for passwd in the manual pages.\nThe apropos The man -k [keyword] command is similar to a help utility called apropos which is available both on Unix and Linux.","title":"Searching for a pattern in the man pages"},{"content":"There are many ways to get help as a Linux administrator, manual pages are one of them as they are always close - accessible via the terminal.\nThe manual pages, called \u0026ldquo;man pages\u0026rdquo; is a local documentation and description of software packages, drivers, routines, and libraries on a Linux machine.\nTo use it run man [command|library|routine|driver] and replace the command with the name of a command to find a manual.\nAn example man whereis shows the manual pages for the whereis command\nMan Pages exist in sections and are supported by both FreeBSD and Linux.\nBelow is a list of supported sections:\nSection Description 1 User-level commands and applications 2 System calls and kernel error codes 3 Library calls 4 Device drivers and network protocols 5 Standard file formats 6 Games and demonstrations 7 Miscellaneous files and configurations 8 Obscure kernel specifications and interfaces To get help for a keyword at a section run\nman [section] keyword\nTo know about the init kernel call, man 8 init\nCiao\n","permalink":"https://limistah.dev/posts/understanding-man-pages/","summary":"There are many ways to get help as a Linux administrator, manual pages are one of them as they are always close - accessible via the terminal.\nThe manual pages, called \u0026ldquo;man pages\u0026rdquo; is a local documentation and description of software packages, drivers, routines, and libraries on a Linux machine.\nTo use it run man [command|library|routine|driver] and replace the command with the name of a command to find a manual.","title":"Know the man(nual) pages"},{"content":"Communication between service workers and the clients browser window can be achieved by simply doing:\nself.clients.matchAll().then((clients) =\u0026gt; { clients.forEach((client) =\u0026gt; client.postMessage({ msg: \u0026#34;Hello from SW\u0026#34; })) }) The variable self is a reserved keyword in a service worker context. It references the global scope of the current worker execution scope and has some useful properties. It is like the window object of a JavaScript browser context.\nIn the above snippet, all the clients that run the service worker are loaded, then the .postMessage is called to send message directly to the original javascript runtime of the service worker.\nThe limitation Sometimes, the clients.matchAll method will return an empty list, meaning that there are no clients for the current service worker, which is actually not true!\nself.clients.matchAll().then((clients) =\u0026gt; { console.log(clients) // [] -\u0026gt; No client, which is not true }) Or event using a waitUntil on an event object:\nconst messaging = firebase.messaging() self.onmessage = (event) =\u0026gt; { event.waitUntil( clients .matchAll({ type: \u0026#34;window\u0026#34;, }) .then(function (clientList) { console.log(clientList) // [] }) ) } And without the client object, it is impossible to send message to a browser JavaScript context.\nA possible solution We can use the BroadcastChannel API to send messages to and from a service worker context from a JavaScript context.\nThe BroadcastChannel API serves like an event Bus inside of a browser. It registers a channel that lives through out the entire lifecycle of the JavaScript runtime, and the channel would be able to send and receive messages regardless of where it is initiated or called.\nBroadcast Channel has a one to many subscription model, there can as many subscribers listening to events from a single channel. Also, the events are only sent and received to scripts executing on same origin, even in different browser tab or browser window.\nUsing BroadcastChannel To create an broadcast channel, initiate the channel then pass the name of the channel for our event broadcasts.\nconst broadCaster = new BroadcastChannel(\u0026#34;sw-messages\u0026#34;) Now, the channel can be used to send a message:\nbroadCaster.postMessage({ message: \u0026#34;Hello from BroadcastChannel\u0026#34; }) The broadcasted message can be consumed with:\nconst receiver = new BroadcastChannel(\u0026#34;sw-messages\u0026#34;) receiver.addEventListener(\u0026#34;message\u0026#34;, function eventListener(event) { console.log(event) }) Using the addEvenListener to listen to the \u0026ldquo;message\u0026rdquo; event, any message broadcasted to the sw-messages channel, would be handled by the code above.\nUse cases BroadcastChannel is not limited to service workers. It is mostly useful when two different parts of an application are running at different contexts but need to pass information across to each other.\nScenarios like, keeping track of changes within a web app running in different tabs, like the logout button click. Also, keeping track of user interactions/updates from a remote server on a web app running in different tabs.\nBrowser compatibility? This feature is part of web standard but still not supported in Safari but has good support in Chrome and Firefox.\nCiao!\n","permalink":"https://limistah.dev/posts/send-message-from-service-worker-broadcastchannel/","summary":"Communication between service workers and the clients browser window can be achieved by simply doing:\nself.clients.matchAll().then((clients) =\u0026gt; { clients.forEach((client) =\u0026gt; client.postMessage({ msg: \u0026#34;Hello from SW\u0026#34; })) }) The variable self is a reserved keyword in a service worker context. It references the global scope of the current worker execution scope and has some useful properties. It is like the window object of a JavaScript browser context.\nIn the above snippet, all the clients that run the service worker are loaded, then the .","title":"Send message from a service worker"},{"content":"First, install CompileDaemon:\n$ go get github.com/githubnemo/CompileDaemon \u0026amp;\u0026amp; go install github.com/githubnemo/CompileDaemon Then, from the root of the project, create a Make file:\n$ touch Makefile And add the below content:\nGOCMD ?= go GOBUILD = $(GOCMD) build GOCLEAN = $(GOCMD) clean GOTEST = $(GOCMD) test GOGET = $(GOCMD) get BINARY_NAME = project_name BINARY_UNIX = $(BINARY_NAME)_unix default: all all: test build build: $(GOBUILD) -o ../$(BINARY_NAME) -v -ldflags=\u0026#34;-X main.VERSION=$(TAG)\u0026#34; test: $(GOTEST) -v ./... clean: $(GOCLEAN) rm -f $(BINARY_NAME) rm -f $(BINARY_UNIX) run: build ./$(BINARY_NAME) dev: CompileDaemon -build=\u0026#34;$(GOBUILD) -o ../$(BINARY_NAME)\u0026#34; -command=\u0026#34;../$(BINARY_NAME)\u0026#34; -color=\u0026#34;true\u0026#34; -exclude-dir=.git -exclude=\u0026#34;.#*\u0026#34; Finally, from the root of your project:\n$ make dev Voila!\n","permalink":"https://limistah.dev/posts/autocompile-go/","summary":"First, install CompileDaemon:\n$ go get github.com/githubnemo/CompileDaemon \u0026amp;\u0026amp; go install github.com/githubnemo/CompileDaemon Then, from the root of the project, create a Make file:\n$ touch Makefile And add the below content:\nGOCMD ?= go GOBUILD = $(GOCMD) build GOCLEAN = $(GOCMD) clean GOTEST = $(GOCMD) test GOGET = $(GOCMD) get BINARY_NAME = project_name BINARY_UNIX = $(BINARY_NAME)_unix default: all all: test build build: $(GOBUILD) -o ../$(BINARY_NAME) -v -ldflags=\u0026#34;-X main.VERSION=$(TAG)\u0026#34; test: $(GOTEST) -v .","title":"Autocompile Go"},{"content":"When programming Erlang, you should think like you are writing an English essay. In Erlang, functions are not very different to what a traditional programming language offers, but they are written very differently in Erlang.\nTo declare a function in the Erlang Repl, you will have to use the fun keyword.\nName = fun(X) -\u0026gt; X. The above code will store the the declaration of a function called Name and would receive an argument called X.\nAs you can see, there is no return keyword to specify the return value. In Erlang everything is an expression and must have a value, to ease working with function, the last expression of the function becomes the value of the function, hence X here is the value of the anonymous function that we defined.\nConsider this basic function declaration in an Erlang Repl\nRectangle_Area = fun({Width, Height}) -\u0026gt; Width * Height end. This is very easy to read right. Building up on the pattern matching lesson, it is clear that the function declaration is doing pattern matching to identify its arguments.\nIf you think like I do, you will see that we can overload function with different arguments. Like this:\nPrice = fun({car}) -\u0026gt; 20; ({bottle}) -\u0026gt; 1; ({fish}) -\u0026gt; .5 end. It should be noticed that we have defined the same function with different arguments. Erlang understands that when we try to call this function, it should do a pattern match against the argument to determine which declaration it should run.\nHence, if Price is called with {car}, it would run the code for Price = fun({car}) -\u0026gt; 20.\nOne subtleness to notice is that when you are doing function overloading, you should separate function definitions with semicolon ; and end the last function with end..\nHigh Order Function is a unique feature of functional programming languages, Erlang ships with this great feature. Erlang functions can be passed as argument, and functions can return another function.\nMap = fun([H|T],F) -\u0026gt; [ F(H) | map(F,T) ], ([], _) -\u0026gt; [] end. %% 2,4,6,8,10,12,14,16,18 Map([1,2,3,4,5,6,7,8,9], fun(X) -\u0026gt; 2 * X end). You should not worry your self with some of the syntax here, we will discuss them very shortly. What you should be concerned with is how we have been able to pass functions to another function call.\nBIFs This is a short acronym for Build In Functions. Erlang ships with a handful of useful built in functions. Check here for a complete list of the BIFs shipped in every erlang installation.\nNow that you have this basic understanding of functions, move up the ladder to know about modules.\n","permalink":"https://limistah.dev/posts/erlang-functions/","summary":"When programming Erlang, you should think like you are writing an English essay. In Erlang, functions are not very different to what a traditional programming language offers, but they are written very differently in Erlang.\nTo declare a function in the Erlang Repl, you will have to use the fun keyword.\nName = fun(X) -\u0026gt; X. The above code will store the the declaration of a function called Name and would receive an argument called X.","title":"Erlang Functions"},{"content":"If you come from a conventional programming language background, the way Erlang handles assignment is expected to look wonky, but it is not.\nThere is nothing like an assignment in Erlang programming language; there is a different approach to accessing values in memory, which is the pattern-matching operations.\nWith Java, PHP, Python, C, C++, and likes, the = symbol implies take the values from the right, and store it into the memory, then give me the reference of the location in memory and store it in the expression at the left.\nSuch that const name = \u0026quot;Aleem Isiaka\u0026quot; is correct in JavaScript, similar constructs exist in other languages using this concept.\nIn Erlang, X = 20 reads very differently.\nThe Erlang runtime Beam understands it as Evaluate the expressions at the right, match it with the value of the expressions at the left.\nClarifying this further, the runtime first evaluates 20, which is a valid Erlang term (integer), then goes to the left to check if the X has a value already; in our case, it does not, meaning the X variable is unbound, an unbound variable will match any value at the right of the match operator (=).\nThis is what happens when we do X = 20. Did you get it?\nThe above example might seem off, let\u0026rsquo;s take this a little bit further. Consider the below Erlang snippet:\nX = 20. Y = 30. Z = 30. X = Y. %% -\u0026gt; Fails, 20 is not equals 30 Y = X. %% -\u0026gt; Fails, 30 is not equals 20 Y = 30. %% -\u0026gt; Works, 30(Y) = 30 30 = Y. %% -\u0026gt; Works, 30 = Y(30) Z = Y. %% Works, 30 = 30 Y = Z. %% Works 30 = 30 %% Works? someFuncThatReturns30 = func() -\u0026gt; 30. Y = someFuncThatReturns30(). %% -\u0026gt; Sure, it does, as long as it returns a value that Y has been bounded to before now or Y is an unbound variable First, we have X matched(bounded, nothing like initialization in Erlang) to 20, with Y and Z bounded to 30. What we should note here is that:\nMatching bounded variables with different values would throw an exception, like where X = Y or Y = X fails the match operation since 20 is not equal 30.\nMatching bounded variables with hardcoded Erlang terms like atoms, binaries integers would match as long as both sides have the same value. This is why an initialized Y is equal to an equivalent of its bounded value 20 Matching variables bounded differently would succeed as long as both variables contain the same value. The return value of a function (func in Erlang) would be successful in matching a variable that holds the same value as the returned value or an unbound variable. For example, an already bounded Y would always check the return value of someFuncThatReturns30 as they are both equal. Yes, that is all to Erlang pattern matching, but you might be wondering, how can I build a robust system if I can\u0026rsquo;t match a variable to another value that it is bounded to? Or do I can run out of names if I keep needing variables. You are not alone; I was here too!\nThe simple answer to this question is that: Erlang is a modular programming language and expects that you write your programs in modules. And modules are a way to group functions in a program.\nThus, in Erlang, variables have lexical scoping and only live in the context/function/process they are declared.\nThis does not make an Erlang program only scalable, and it also makes it more predictable as you don\u0026rsquo;t expect a variable to have two different values in the same context.\nVariable names can be duplicated in different contexts:\nsomeFunc = (X) -\u0026gt; x -\u0026gt; 20; y -\u0026gt; 30. anotherFun = (X) -\u0026gt; all -\u0026gt; 100; half -\u0026gt; 50. someFunc takes in one argument, which is pattern matched against x or y, both of which are atoms to return the requested value. While anotherFunc takes in one argument and pattern matching against all or half to return the requested value.\nWe can notice that both have X as the name of their argument; this is a valid Erlang module code that would be successful if it runs.\nThe operation\u0026rsquo;s success illustrates that we have to be careful of the context we are in not to bind a variable to an already bounded variable and pattern match correctly.\nWith these, I believe you have the basics of Erlang Pattern Matching.\nPattern matching is an excellent tool in writing programs for Erlang and other languages that depend on its VM.\nNow that you have its basic understanding move up the ladder to know about functions.\nNamaste!\n","permalink":"https://limistah.dev/posts/erlang-pattern-matching/","summary":"If you come from a conventional programming language background, the way Erlang handles assignment is expected to look wonky, but it is not.\nThere is nothing like an assignment in Erlang programming language; there is a different approach to accessing values in memory, which is the pattern-matching operations.\nWith Java, PHP, Python, C, C++, and likes, the = symbol implies take the values from the right, and store it into the memory, then give me the reference of the location in memory and store it in the expression at the left.","title":"Erlang Pattern Matching"},{"content":"If you have worked with other languages like JavaScript, Java, Python etc, you would be surprised by what Erlang understands as variable.\nIn Erlang, variables starts with uppercase letter, thus, C, X, Ape, Ant are all valid identifiers for Erlang variables.\nVariables can not start with lowercase letter or begin with a number.\nErlang variables can include can alphanumeric characters, an underscore and @ symbol.\nX = 1. %% Valid y = 2. %% Invalid 1X = 2. %% Invalid When assigning to a variable - as we do call it in other languages, Erlang actually does something called pattern matching. Comparing the values on the right to the values on the left.\nA pattern match would only succeed if the two operands match.\nThus, in Erlang, a variable get assigned to if it is either unbound or has the same value as the value at the right.\nThe = is a special symbol that does not do assignment but makes the pattern matching operation to be successful if its conditions are met.\nHence, in Erlang = is known as a pattern match operator which evaluates the value of the right hand side (RHS) then matching the result with the left hand side (LHS)\nX = 1. %% successful Y = 2. %% successful Z = X + 1. %% successful, Z is unbound to Z. %% =\u0026gt; 3 Z = 3 %% successful, Z is now 3, but, we want it to have the value 3 Z = 4 %% fails, Z is already holding 3, and 4 won\u0026#39;t match against it. There is nothing as global or private scope in Erlang, all variables are lexically scoped and are unrelated even if they exists in different functions;\nContinue to the next post on Erlang Learning: Erlang Pattern Matching\nCiao!\n","permalink":"https://limistah.dev/posts/erlang-variables/","summary":"If you have worked with other languages like JavaScript, Java, Python etc, you would be surprised by what Erlang understands as variable.\nIn Erlang, variables starts with uppercase letter, thus, C, X, Ape, Ant are all valid identifiers for Erlang variables.\nVariables can not start with lowercase letter or begin with a number.\nErlang variables can include can alphanumeric characters, an underscore and @ symbol.\nX = 1. %% Valid y = 2.","title":"Erlang Variables"},{"content":"Without JavaScript, dynamic UI is possible with just CSS action class selectors.\nCase study Help text for input element which is only visible when the input is focused.\nThe HTML \u0026lt;div class=\u0026#34;input-cont\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; placeholder=\u0026#34;Focus me\u0026#34; autofocus /\u0026gt; \u0026lt;span class=\u0026#34;help\u0026#34; data-help=\u0026#34;Enter your email\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; To achieve the above, a span holding the help text in a data-help attribute as a sibling to the actual input. Both the input and the span are children to a parent div with class name input-cont.\nThe interaction(CSS) The container would help to proper place the content, so a position relative is applied. Since multiple inputs can exist on a page, a margin-bottom is added to give a visual cue and separation.\n.input-cont { position: relative; margin-bottom: 1rem; } input { z-index: -1; } The Structure Instead of additional element to hold the text, the ::before pseudo element is already available for container elements.\n\u0026lt;input /\u0026gt; elements are not container which makes it difficult for this purpose, a span is placed next to the input element to provide the flexibility that ::before, ::after pseudo elements provide.\nThe +(direct sibling) selector can achieve selecting an element that lives under the same parent as the base selector. Since the input and the span.help lives under the same parent, so input+.help gets the span.help element, and the ::before gives access to its pseudo selector.\nTo ensure that the ::before pseudo element stay behind the input field, the z-index: -2 is applied to it, also, the content property is not set at the initial state of the element, making the element hidden by default.\ninput + .help::before { bottom: 50%; left: 0; width: 100%; font-size: 0px; color: #707070; opacity: 0; position: absolute; filter: blur(5px); z-index: -2; } Toggling the help text To show the help text when an element is hidden, the :focus psuedo selector is used to select an input that is focused, then the sibling selecor gives access to the span.help element, which makes the span.help::before pseudo element stylable.\nHere, the content is set by picking it from the data-help attribute on the span.help element, a final z-index: 0 is applied to bring the span.help::before element on top of the input field, while the bottom: 60% pushes the element downward below the input field to avoid overlapping.\ninput:focus + .help::before { content: attr(data-help); opacity: 1; filter: blur(0); bottom: -60%; z-index: 0; font-size: 12px; } Find the above code on codepen.io\nAnd this is just a basic toggling and hiding of elements using :focus.\nMany interesting things can be built with CSS user action selectors.\nThere are other action selectors are:\n:active :focus :focus-within :focus-visible :hover Salut!\n","permalink":"https://limistah.dev/posts/css-action-pseudo-classes/","summary":"Without JavaScript, dynamic UI is possible with just CSS action class selectors.\nCase study Help text for input element which is only visible when the input is focused.\nThe HTML \u0026lt;div class=\u0026#34;input-cont\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; placeholder=\u0026#34;Focus me\u0026#34; autofocus /\u0026gt; \u0026lt;span class=\u0026#34;help\u0026#34; data-help=\u0026#34;Enter your email\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; To achieve the above, a span holding the help text in a data-help attribute as a sibling to the actual input. Both the input and the span are children to a parent div with class name input-cont.","title":"CSS action pseudo classes"},{"content":"If you have used WhatsApp or Facebook Chat, then you have one way or the other interacted with an Erlang-backed system.\nErlang is a language created for the telecommunication industry by Jor Armstrong, Robert Virding, and Mike Williams in 1986. It was recorded that Jor Armstrong claimed he was provided a library and did not know what to do with it, then they taught him to solve the reliability and concurrent problem of the telecommunication industry, and that gave birth to Erlang.\nThe strength of Erlang lies in running scalable applications in a Distributed environment. It allows computers to network with each other with a very little overhead on the programmer and the operating system.\nIn this post, we will be getting a formal introduction to Erlang and we are just covering the basics. We would be covering variables, funcs (functions), modules, types, records, maps, processes, and distributed systems.\nThe whole of the OTP framework won\u0026rsquo;t be covered. In the future, we will be having a post about this!\nInstallation and Initialization For a thorough guide on how to install Erlang runtime on different platforms, check out Bruce Yinhe\u0026rsquo;s post on Medium about this.\nAfter a successful installation, open a terminal/CMD, and type in erl, a default welcome message and prompt should be seen. Great, welcome to the Erlang world.\nNow that we have covered the basic installation, follow the links below to learn more about Erlang and its syntax.\nVariables Pattern Matching Funcs Modules Types Records Maps Processes Processes communication Distributed system ","permalink":"https://limistah.dev/posts/an-introduction-to-erlang/","summary":"If you have used WhatsApp or Facebook Chat, then you have one way or the other interacted with an Erlang-backed system.\nErlang is a language created for the telecommunication industry by Jor Armstrong, Robert Virding, and Mike Williams in 1986. It was recorded that Jor Armstrong claimed he was provided a library and did not know what to do with it, then they taught him to solve the reliability and concurrent problem of the telecommunication industry, and that gave birth to Erlang.","title":"An introduction to Erlang"},{"content":"CSS can keep count of numbers without writing any additional JavaScript.\nIt does this by taking note the amount of time a CSS block affects a page then incrementing the counter for that block if the counter-increment rule is implemented.\nFor example:\ninput:invalid { counter-increment: invalid-count; } With no JavaScript at all, CSS understands that whenever there is an invalid element, it should increment the count for the invalid-count identifier.\nStructure of CSS counters CSS counter is a CSS rule that specify the increment-counter property and an indentifier for the counter to increment.\nTo know how many HTML exists in a page:\nh1 { increment-counter: total-h1-counter; } Since CSS would match every h1 on the page, the increment-counter would be executed, thereby increasing the count for the total-h1-counter.\nGetting back to 0 A counter can go back to the 0 state by declaring the counter-reset: counter-identifier CSS rule\nA use case here would be always reseting the number of total-h1-counter on the page when the body of the document is matched, but incrementing when H1s are matched.\nThe CSS below helps to achieve that:\nbody { counter-reset: total-h1-counter; } h1 { counter-increment: total-h1-counter; } Rendering counter on a page. We can use CSS Generated Content to make the value of a counter visible on the page. We can achieve this using pseudo elements(::before, ::after) through the content property.\n\u0026lt;h1\u0026gt;The first\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;The Second\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;The third\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;The fourhtb\u0026lt;/h1\u0026gt; \u0026lt;section\u0026gt;There are\u0026lt;/section/\u0026gt; body { counter-reset: total-h1-counter; } h1 { counter-increment: total-h1-counter; } section:after { content: \u0026#34; \u0026#34; counter(total-h1-counter) \u0026#34; H1 tags on the page\u0026#34;; font-weight: bold; color: tomato; } Salut!\n","permalink":"https://limistah.dev/posts/css-counters/","summary":"CSS can keep count of numbers without writing any additional JavaScript.\nIt does this by taking note the amount of time a CSS block affects a page then incrementing the counter for that block if the counter-increment rule is implemented.\nFor example:\ninput:invalid { counter-increment: invalid-count; } With no JavaScript at all, CSS understands that whenever there is an invalid element, it should increment the count for the invalid-count identifier.","title":"CSS counters"},{"content":"Yeah, you read that right. Let go straight into it!\nA Test Case Can you interprete this CSS selector query?\np.title:first-of-type { color: red; } Let me think like you would:\nSelect every P element that has the class name of title and apply the color red to the first of its type.\n\u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Lorem Ipsum\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;Paragraph 2 (shows in red color)\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; You think you’re right.\nAnother test case What happens with the below:\n\u0026lt;style\u0026gt; p.paragraph-1:first-child { color: green; } \u0026lt;/style\u0026gt; \u0026lt;div\u0026gt; \u0026lt;p class=\u0026#34;paragraph-1\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;span1\u0026#34;\u0026gt;This is the title\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;span2\u0026#34;\u0026gt; Shouldn\u0026#39;t be the title \u0026lt;/span\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; You would expect the SPAN with the class name span1 should have the color green.\nTo you, the query reads:\nSelect the first child of every P tag that has a class name of paragraph-1,\nWell,that is not correct!\nThe query correctly reads:\nSelect every P tag with a class name of paragraph-1 and IT IS THE FIRST CHILD of its parent.\nKnow the Strcuture To better understand Structural selectors in CSS, we have to consider the query first and then attach the selector\u0026rsquo;s semantic meaning.\n:only-child should read as AND IS THE ONLY CHILD, but not seeing it as going deeper to the actual element\u0026rsquo;s children.\nStructural selectors always refer to the position of an element itself in a document.\n\u0026lt;p\u0026gt; \u0026lt;h1\u0026gt;1\u0026lt;/h1\u0026gt; \u0026lt;h1\u0026gt;2\u0026lt;/h1\u0026gt; \u0026lt;/p\u0026gt; For the structure above, h1:first-child would pick the first h1, h1:last-child would select the second h1.\nThe beauty of structural selectors is that page structures are stylable regardless of their complexity.\nFor example, the only child of an element, empty elements, any element that is a child to an element but existing at an index, and more are stylable, which is powerful!\nRef List Reference the list below to understand structural selectors proper:\n\u0026lt;selector\u0026gt;:first-child - Will match any element that is the first child \u0026lt;selector\u0026gt;:last-child - will match any element that is the last child \u0026lt;selector\u0026gt;:only-child - will match any element that is the only child of its element \u0026lt;selector\u0026gt;:nth-child(n) - will match any element that is a child of a parent element but occurring at the index of n for that selector \u0026lt;selector\u0026gt;:nth-last-child(n) - will match any element that is a child element occurring at the index of n for that selector. Take it as `nth-child moving in reverse mode. \u0026lt;selector\u0026gt;:empty - will match any element that is empty or has just comments \u0026lt;selector\u0026gt;:first-of-type - will match any element, and it is the first occurrence for that selector \u0026lt;selector\u0026gt;:last-of-type - will match any element that is the last occurrence for that selector \u0026lt;selector\u0026gt;:nth-of-type(n) - will match any element occurring at index of n for that selector \u0026lt;selector\u0026gt;:nth-last-of-type(n) - will match the last element for every element occurring at the index of n for that selector. Take it as nth-type moving in reverse mode. \u0026lt;selector\u0026gt;:only-of-type - will match the only element that has for that selector. Interesting right? Play with it here.\nTake home: Structural selectors apply themselves to the selected element to help us style an element existing at a specific position in the HTML.\nau revoir!\n","permalink":"https://limistah.dev/posts/structural-css-selectors/","summary":"Yeah, you read that right. Let go straight into it!\nA Test Case Can you interprete this CSS selector query?\np.title:first-of-type { color: red; } Let me think like you would:\nSelect every P element that has the class name of title and apply the color red to the first of its type.\n\u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Lorem Ipsum\u0026lt;/p\u0026gt; \u0026lt;p class=\u0026#34;title\u0026#34;\u0026gt;Paragraph 2 (shows in red color)\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; You think you’re right.\nAnother test case What happens with the below:","title":"Structural CSS Selectors"},{"content":" Open ios/{APP_NAME}/Info.plist.\nAdd\n\u0026lt;plist\u0026gt; \u0026lt;dict\u0026gt; ... ... ... \u0026lt;key\u0026gt;UIBackgroundModes\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;audio\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; Press r on the metro terminal Voila!\n","permalink":"https://limistah.dev/posts/enable-background-audio-react-native/","summary":"Open ios/{APP_NAME}/Info.plist.\nAdd\n\u0026lt;plist\u0026gt; \u0026lt;dict\u0026gt; ... ... ... \u0026lt;key\u0026gt;UIBackgroundModes\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;audio\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt; Press r on the metro terminal Voila!","title":"How to enable background Audio Play in iOS React Native"},{"content":" npm install rollup-plugin-node-resolve rollup-plugin-json.\nAdd it to the plugins inside rollup.config.js\n// Rollup configuration ... plugins: [ rollupNodeResolve({ jsnext: true, preferBuiltins: true, browser: true }), rollupJson(), ... ] ** Notice the browser: true **\nRun your build again: yarn run build Voila!\nSource\n","permalink":"https://limistah.dev/posts/configure-axios-rollup/","summary":"npm install rollup-plugin-node-resolve rollup-plugin-json.\nAdd it to the plugins inside rollup.config.js\n// Rollup configuration ... plugins: [ rollupNodeResolve({ jsnext: true, preferBuiltins: true, browser: true }), rollupJson(), ... ] ** Notice the browser: true **\nRun your build again: yarn run build Voila!\nSource","title":"Configure Rollup to bundle Axios module"},{"content":"To make Application Icons for both iOS and Android, use https://appicon.co/\nThe advantage of appicon.co is that after adding the base icon image, it generates the set of standard icons, and automatically downloads them to a ZIP file named: AppIcons.\nExtract the zip archive, it contains\nappstore.png: For Apple AppStore playstore.png: For Google playstore android/: A folder of standard Icons for Android Assets.xcassets: A folder of standard Icons for iOS ","permalink":"https://limistah.dev/posts/make-app-icon/","summary":"To make Application Icons for both iOS and Android, use https://appicon.co/\nThe advantage of appicon.co is that after adding the base icon image, it generates the set of standard icons, and automatically downloads them to a ZIP file named: AppIcons.\nExtract the zip archive, it contains\nappstore.png: For Apple AppStore playstore.png: For Google playstore android/: A folder of standard Icons for Android Assets.xcassets: A folder of standard Icons for iOS ","title":"How to create Application Icon (Appstore | Playstore)"},{"content":"Write Here\n","permalink":"https://limistah.dev/posts/shiny-cutting-edge/","summary":"Write Here","title":"Title of the post"},{"content":"Introduction Docker is an app development tool that eases the process of creating, running, and deploying applications. It uses the concept of containers which work just like a Virtual Machine does.\nWhile Docker runs more like a Virtual Machine does, it is more advantageous than a VM.\nIt let us define OS-like images like we are writing an actual OS that includes the only tools that we need, aside this, Docker utilizes the concept of layers which makes its images very much extensible. With this little feature, developers, sysadmins and devops engineers prefer it more. And since it has been in existence, Docker has witnessed widespread usages making it one of the defacto tool for software development, testing and delivery.\nWhile there are many low level details about Docker which we won\u0026rsquo;t be doing in this post. In this post, we will focus on creating a docker compose file that could ease the development and deployment of NodeJS based web applications which require an nginx server which acts as a proxy server to a NodeJS application, MongoDB as the database and Redis as its dependencies.\nBase Dockerfile First, we will define a Dockerfile this is a file that Docker reads to create an image which could be used to start a container.\nThis file has some interesting commands but we are only interested in a few. This file should look like below:\n# Base Image. # This image already exists in dockerhub. # We are just extending its functionalities FROM node:10 # Already created by the node image USER node # Changes directory in the container to this directory WORKDIR /home/node/app # Port that the node app will listen to EXPOSE 8000 The above Dockerfile commands is what we can use when we are defining an image. Images are typical to installing a fresh operating system, instead of having a full OS on a different machine, Docker builds this image to work with any computer without installing it.\nAn operating system image inside of another OS is not new, it could be found in Virtual Machine implementations. What they do is utilize a feature of computers called HyperV, it is\nSo, in our Dockerfile, we are calling a few commands that Docker could understand to create a new image for us. A brief explanation about the used commands:\nFROM: It inherits from another image, builds untop of it, making it easier to make modifications. USER: Sets the user that is associated with the current session inside of an interactive terminal in case we need one. WORKDIR: Sets the default directory that a terminal would be when it is initialized. EXPOSE: The image can receive connections, but could only be connected to by containers in the same network, except it is told to expose the port to the host machine. This command exposes a port of an image to its host machine. So, this is complete in our case. An OS with NodeJS, and we know a directory to put our source codes inside of the image, interesting.\nNext, we have to build this image to confirm if we the Dockerfile contains a valid syntax.\nTo build a Docker Image from a directory that has a Dockerfile - like ours, we can run:\ndocker image build -t nodejs This would pull the NodeJS image from hub.docker.com then run the remaining commands to modify for our new image. When this is done, we could verify by running:\ndocker image ls The above command would list the images currently installed locally.\nIf that works, we have successfully build our nodejs image, and we can now create containers that uses the images.\nContainers are like the computer itself, they run using an image just like our physical computers run using an operating system. In Docker, think of Images like an installable operating system, while containers to be the machine that can install an operating system, in this case an image.\nTo have a container running using the nodejs image, we can use the below command:\ndocker container start -it --name nodejs-container nodejs The command above is starting a container, gives the container a name of nodejs, run it interactively, then use the nodejs image that we built earlier.\nThe result of the command above is a nodejs repl being launched in our terminal.\nWe are seeing that because that is what the parent image(nodejs) specified. In its Dockerfile it specifies CMD: node. We can override this easily in our docker file, but that is not a good idea, we want this setup to be reusable so we should leave that as it is. There is a better way to override it which would not require modifying the Dockerfile.\nNginx Dockerfile As a rule of thumb, we can only have one Dockerfile in a directory, and we have used that opportunity to make that file build a NodeJS image for us. But, we also need to build an nginx image from a parent image on DockerHub which would receive http requests like our webserver would.\nWhy we need an Nginx image is to bootstrap an Nginx server that would mimic our production live server. What Nginx server does in NodeJS deployments is to create a proxy against the actual node server from the rest of the world.\nLocal User \u0026ndash;\u0026gt; Our Web Server \u0026ndash;\u0026gt; Node App inside the server.\nLocal User \u0026lt;\u0026ndash; Our Web Server \u0026lt;\u0026ndash; Node App inside the server\nFor us to achieve this architecture Dockerized, it is essential we build an image off the official nginx image from DockerHub. Let\u0026rsquo;s do this!\nLet\u0026rsquo;s create a file named nginx.Dockerfile in the same directory, and have the content as below:\nFROM nginx:1.13 COPY nginx.conf /etc/nginx/conf.d/default.conf This is quite smaller to that of the NodeJS, but would do a full build process of the nginx image. This is the power of Docker, when we extend an image, we can add special functionalities to the image, which leaves us from the low-level details that is associated with creating that image.\nIn this nginx Dockefile, we are extending the official nginx image at version 1.13, then copying a file nginx.conf to the default nginx config path. The copy command is to ensure that we make our nginx do what we want when we are ready to create our proxy server.\nAwesome!\nNext, we can build this image as we have built the NodeJS\u0026rsquo; to very our syntaxt. But before we build, let\u0026rsquo;s add a file at the same location named nginx.conf. This file should have the following content:\nserver { listen 80; location / { proxy_pass http://localhost:5000; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $server_name; } } As should be noticed in the nginx.conf file, we are redirecting all traffic coming to this server to another address(http://localhost:5000), we are assuming an app is running at this port. Now we can build the nginx Dockerfile to a Docker image with:\ndocker image build -t nginx -f nginx.Dockefile We have to specify the -f nginx.Dockerfile to tell Docker to not use the default Dockerfile which is for our nodejs image.\nAnd running a container off of the image:\ndocker container start -it --name nginx-container -p 80:80 nginx Perfect! We now have a proxy nginx server that routes request to its 127.0.0.1:5000. As we know, images are OS of their own, and the nginx would resolves localhost to itself, we don\u0026rsquo;t want it to route to itself since our nodejs app would live in the nodejs container. We will later provide a solution to this.\nMongoDB and Redis Dockerfiles We won\u0026rsquo;t be building an image for MongoDB and Redis, the major reason is that we don\u0026rsquo;t have any customization to add to the image that we would be inheriting from. So, it is more of over-engineering to have a dockerfile that contains:\nFROM mongodb:alpine When we can have a docker-compose file to take away that for us or create a container on the image which would be pulled from dockerhub.\nDocker-Compose Composability is one of the many features that Distinguishes Docker from a VM. From Compose we should understand that we can create many images, containers, networks, drives on the fly in just one file. This helps with maintainance and helps to conceptualize software as a mixture of dependencies in their uniqueness.\nSo, let\u0026rsquo;s compose an architecture for NodeJS, Nginx, Mongo and Redis!\nIn the same directory, create a file named docker-compose.yml, compose files are YAML files with commands that the docker-compose command understands.\nOnce that file is created, let\u0026rsquo;s define some images that we would be needing for our architecture. First, I will define the nodejs image, making the first version of docker-compose.yml file to look like this:\nversion: \u0026#34;2\u0026#34; services: node: build: context: . dockerfile: Dockerfile volumes: - ./:/home/node/app command: \u0026#34;npm start\u0026#34; ports: - 5000:5000 In the snippet above, we are trying to create a node image through this compose file from the Dockerfile we defined above, (the context is the path to look for, while the dockerfile is a customized name for our Dockerfile, we can omit it since our nodejs Dockerfile has the default naming convention, but, we have added it for clarity sake).\nAlso, we could notice we were setting some information, if you can remember when how we create containers: docker container run --name nginx -p 80:80 nginx, we can pass parameters and further extend the image through these commands.\nHere we can see that we have forwared port 5000 from the container to the container\u0026rsquo;s 5000, map the current directory to the /home/node/app folder in the container and how we could specify the command we want to run after the container might have been started? That is the power of compose files. To know more about the commands that can be passed, see the docker-compose documentation\nNow testing our compose file, we can start the compose file in docker and not forget, as docker to build us the nodejs image since we have specified the build command:\ndocker-compose up --build That is it, we have our first docker-compose running!\nNext, we should include our nginx config into the docer-compose file. Which would make our docker-compose.yml to look like this:\nversion: \u0026#34;2\u0026#34; services: node: build: context: . dockerfile: node.Dockerfile volumes: - ./:/home/node/app command: \u0026#34;npm start\u0026#34; ports: - 5000:5000 networks: # notice this addition to the node servic - nginx-proxy # notice this - default # notice this nginx: build: context: . dockerfile: nginx.Dockerfile volumes: - ./:/home/node/app # command: \u0026#34;npm start\u0026#34; restart: always ports: - 80:80 depends_on: - node networks: - nginx-proxy - default networks: nginx-proxy: With this update, we have included the nginx.Dockerfile to build an nginx image for us which should include the nginx.conf we have. To be noticed in the bginx service is depends_on , this commands ensures that the node is started and running before nginx is started. We also added the both services to the same networks so they can communicate using their service name instead of their respective virtual ips.\nFor ngix.conf to work, we have to update the part that redirects to localhost:5000 to redirect to http://node:5000. The node hostname would be resolved to virtual ip address by Docker during runtime. This change should make our nginx.conf to look like this:\nserver { listen 80; location / { proxy_pass http://node:5000; # updated from localhost to the service name proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $server_name; } } Awesome, we can build these two services and run them at the same time as we did before using: docker-compose up --build. Both services running? Great!\nIf the node service is failing, do init a node project in the root directory with: npm ini -y and add a start script OR remove the command: npm start from the docker-compose.yml file.\nWe are doing well!\nNext let\u0026rsquo;s include Redis service into the compose file.\nversion: \u0026#34;2\u0026#34; services: node: build: context: . dockerfile: node.Dockerfile volumes: - ./:/home/node/app command: \u0026#34;npm start\u0026#34; ports: - 5000:5000 depends_on: - mongodb-server networks: - nginx-proxy - default nginx: build: context: . dockerfile: nginx.Dockerfile volumes: - ./:/home/node/app restart: always ports: - 80:80 depends_on: - node # network_mode: bridge networks: - nginx-proxy - default redis-server: image: \u0026#34;redis:5.0.8-alpine\u0026#34; ports: - \u0026#34;6379:6379\u0026#34; networks: nginx-proxy: It gets more interesting, but simply done. We don\u0026rsquo;t need any special configuration or extension for the Redis Image. So, we just use the image: ... command instead of the build command to tell the image we want to use. This also means that docker will not build this image, rather download it from Dockerhub.\nFinally, a database. Let\u0026rsquo;s add mongodb service to complete our setup.\nversion: \u0026#34;2\u0026#34; services: node: build: context: . # You can rename the file to make it tell what it does. # since we are composing different images and not just a sigle image dockerfile: node.Dockerfile volumes: - ./:/home/node/app command: \u0026#34;npm start\u0026#34; ports: - 5000:5000 depends_on: # We now depends on node and mongodb server - redis-server - mongodb-server networks: - nginx-proxy - default nginx: build: context: . dockerfile: nginx.Dockerfile volumes: - ./:/home/node/app restart: always ports: - 80:80 depends_on: - node networks: - nginx-proxy - default redis-server: image: \u0026#34;redis:5.0.8-alpine\u0026#34; ports: - \u0026#34;6379:6379\u0026#34; mongodb-server: image: \u0026#34;mongo:4.2.3-bionic\u0026#34; ports: - \u0026#34;27017:27017\u0026#34; networks: nginx-proxy: As simple as the redis service is, same with the mongodb service. We are downloading the image from github and running a mongodb-server container off it.\nGreat work!\nNow, we can test our setup as we always do: docker-compose up --build.\nSpeaking between containers. If we try to connect to the redis service from our node app we might get an error as the address 127.0.0.1 would mean the instance itself and not the host computer, same will happen with the mongodb service.\nTo solve this, we will add a service_name to every service we want to reference inside of another container, and ensure that the depends_on is updated to that new service name.\nSo our final config would look like this:\nversion: \u0026#34;2\u0026#34; services: node: server_name: node build: context: . # You can rename the file to make it tell what it does. # since we are composing different images and not just a sigle image dockerfile: node.Dockerfile volumes: - ./:/home/node/app command: \u0026#34;npm start\u0026#34; ports: - 5000:5000 depends_on: # We now depends on node and mongodb server - redis-server - mongodb-server networks: - nginx-proxy - default nginx: service_name: nginx build: context: . dockerfile: nginx.Dockerfile volumes: - ./:/home/node/app restart: always ports: - 80:80 depends_on: - node networks: - nginx-proxy - default redis-server: service_name: redis image: \u0026#34;redis:5.0.8-alpine\u0026#34; ports: - \u0026#34;6379:6379\u0026#34; mongodb-server: service_name: mongodb image: \u0026#34;mongo:4.2.3-bionic\u0026#34; ports: - \u0026#34;27017:27017\u0026#34; networks: nginx-proxy: Easy enough? Now in our source codes we can use http://redis:6379 to connect to redis service container and http://mongodb:27017 to connect to the mongodb service container.\nVoila!\nConclusion Finally, we cracked it. Walking through the journey from an empty file to a docker-compose.yml file that we can share with anyone that would love to replicate same setup for their development. This was so much fun to walk through.\nThere is more to Docker than we have covered, don\u0026rsquo;t hesitate to read the docs for more information and commads that could be used in Dockerfile and docker-compose.yml files.\nPLEASE!!!\nDockerize yourself at this time, stay safe, stay home, stay in a containerized environment!!!\nOu re voir!\n","permalink":"https://limistah.dev/posts/docker-node-nginx-redis-mongodb/","summary":"Introduction Docker is an app development tool that eases the process of creating, running, and deploying applications. It uses the concept of containers which work just like a Virtual Machine does.\nWhile Docker runs more like a Virtual Machine does, it is more advantageous than a VM.\nIt let us define OS-like images like we are writing an actual OS that includes the only tools that we need, aside this, Docker utilizes the concept of layers which makes its images very much extensible.","title":"Dockerized Node/Nginx, MongoDB, Redis app setup"},{"content":"Introduction SocketIO is a JavaScript library that makes developers\u0026rsquo; lives easier when dealing with web socket and socket programming. This is the fact that SocketIO has abstracted out all the low-level and tedious steps that are associated with setting up a socket server and client; it has made the question of programmers be \u0026ldquo;How can I structure my application.\u0026rdquo;\nWhile I have done different types of socket implementations, I will walk us through a setup that has always work for me and has proven to be the best in cases that I have had to use SocketIO.\nIn this post, we will be implementing a basic SocketIO server, set up a small database for our users, have a client that consumes our application.\nIt will never be as dull as you think, I promise.\nI have created a repo for the setup and could be found here.\nInstallation \u0026amp; bootstrapping So, to begin with, I will initialize a new repository for the setup for clarity reasons.\nmkdir socketio-setup cd ./socket-io-setup git init npm init -y In the above code snippet, we are trying to bootstrap our folder structure and codebase. The first line creates a new directory in our local hard drive. We changed the current directory into the newly-created directory. We initialized a new but empty GitHub repo. In the last line, we initialized a new npm project using npm init -y and accepting the default config through the -y flag.\nTo further complete our initializations, we will install socketIO, add .gitignore to exclude some noisy folders, and add our first commit so far for our new repo, smiles.\nnpm install -S socketio express touch .gitignore echo \u0026#39;/node_modules\u0026#39; \u0026gt; .gitignore git add . git commit -am \u0026#34;Initial commit\u0026#34; After completing our project folders\u0026rsquo; initialization, we should attempt to bootstrap a basic socket server. To accomplish this, we will first create a folder called src in the root of the project and make index.js the sole file in the folder. Having done that, we can add some code into the src/index.js file.\nWe will first of all create a basic express server and initialize socketIO library:\n// Copied from https://socket.io/get-started/chat/ var app = require(\u0026#34;express\u0026#34;)() var http = require(\u0026#34;http\u0026#34;).createServer(app) var io = require(\u0026#34;socket.io\u0026#34;)(http) app.get(\u0026#34;/\u0026#34;, function (req, res) { res.send(\u0026#34;\u0026lt;h1\u0026gt;Socket IO project folder setup\u0026lt;/h1\u0026gt;\u0026#34;) }) io.on(\u0026#34;connection\u0026#34;, function (socket) { console.log(\u0026#34;a user connected\u0026#34;) }) http.listen(3000, function () { console.log(\u0026#34;listening on *:3000\u0026#34;) }) And with the above, we have successfully created a basic socketIO server. Hurray!\nEvents \u0026amp; Listeners Events and Listeners are two of the basic concepts that are significantly related to SocketIO library.\nListeners With Listeners the client(s) could tell the server that something should happen in the server. A basic example of a listener is when a user is connected or disconnected:\n// Connection event io.on(\u0026#34;connection\u0026#34;, function (socket) { console.log(\u0026#34;User with socketId %s connected\u0026#34;, socket.id) }) // Disconnection event io.on(\u0026#34;disconnect\u0026#34;, function (socket) { console.log(\u0026#34;User with socketId %s disconnected\u0026#34;, socket.id) }) Simply put, a listener is a block of code that a client tells the server to run after the server might have registered the listener with a name.\nTo register a listener, we only have to call the .on function on the io object. The very many ways to call this function are properly documented on the socketio\u0026rsquo;s website.\nEvents Somethings might happen on the server that it might be so exciting to want to tell a client. When we decide that a client should know of something, we are triggering an event.\nsocket.emit(\u0026#34;hello\u0026#34;, \u0026#34;can you hear me?\u0026#34;, 1, 2, \u0026#34;abc\u0026#34;) We emit on the socket, as that socket needs to know about the event that just happened. Nevertheless, we can emit any socket, sockets or room space, etc., and emit cheatsheet exists for this purpose.\nApplication folder structure Having understood the basic concepts of Events and Listeners, it is so glaring that we can have all of the listener and events in just a file say the src/index.js file.\nOur code could be messed up and look more like this\n//... // Listener 1 io.on(\u0026#34;someEventName_1\u0026#34;, function (socket) { console.log(\u0026#34;someEventName_1 with socketId: %s\u0026#34;, socket.id) socket.emit(\u0026#34;someEventName_1\u0026#34;, { message: \u0026#34;Success\u0026#34; }) //... }) // Listener 2 io.on(\u0026#34;someEventName_2\u0026#34;, function (socket) { console.log(\u0026#34;someEventName_2 with socketId: %s\u0026#34;, socket.id) socket.emit(\u0026#34;someEventName_2\u0026#34;, { message: \u0026#34;Success\u0026#34; }) //... }) // Listener 3 io.on(\u0026#34;someEventName_3\u0026#34;, function (socket) { console.log(\u0026#34;someEventName_3 with socketId: %s\u0026#34;, socket.id) socket.emit(\u0026#34;someEventName_3\u0026#34;, { message: \u0026#34;Success\u0026#34; }) //... }) //... Interesting to note is that some handlers for some listeners can get so large. So, what do we do?\nThis is what we are trying to solve in this post. Firstly, let\u0026rsquo;s create a folder for our listeners and add an index.js file into it.\n\u0026ldquo;`bash mkdir src/listeners touch ./src/listeners/index.js\nThe `src/listeners` have distinct modules that export just a function and accept an `io` parameter, the socketIO object. The index file will be responsible for the initialization of these modular events files. Moving the `connection` event to an event module would in a file located at `src/listeners/connection.js`, look like this: ```js module.exports = function (io) { io.on(\u0026#34;connection\u0026#34;, function (socket) { socket.emit(\u0026#34;connected\u0026#34;, socket) }) } In the above module, we are exporting a function that accepts the io parameter. In the listener\u0026rsquo;s body, we are trying to tell the socket that it has connected by emitting a connected event. Simple enough!\nMoving forward, we can now import this new listener module in the index.js at the listener\u0026rsquo;s directory; then, we can write our bootstrapping code for the listener.\n// src/listeners/index.JS module.exports = (io) =\u0026gt; { const fs = require(\u0026#34;fs\u0026#34;) const path = require(\u0026#34;path\u0026#34;) // Full path to the current directory const listenersPath = path.resolve(__dirname) // Reads all the files in a directory fs.readdir(listenersPath, (err, files) =\u0026gt; { if (err) { process.exit(1) } files.map((fileName) =\u0026gt; { if (fileName !== \u0026#34;index.js\u0026#34;) { console.debug(\u0026#34;Initializing listener at: %s\u0026#34;, fileName) // Requires all the files in the directory that is not a index.js. const listener = require(path.resolve(__dirname, fileName)) // Initialize it with io as the parameter. listener(io) } }) }) } In the above code, we are only trying to ensure that all files in the listener directory are required and run with an io object as the parameter. The whole src/listeners/index.js is being exported as a function to ensure that we only run the module when we need and that the io parameter is being passed down.\nWith this arrangement, subsequent listeners would only require us to create a file inside the src/listeners directory, function as the main export, and accept io as the sole parameter.\nNext, we have to import the src/listeners/index.js in the src/index.js file. To do this, the src/index.js file will look like this.\n// Requires the listener directory(index.js file) const initListeners = require(\u0026#34;./listeners\u0026#34;) var app = require(\u0026#34;express\u0026#34;)() var http = require(\u0026#34;http\u0026#34;).createServer(app) var io = require(\u0026#34;socket.io\u0026#34;)(http) app.get(\u0026#34;/\u0026#34;, function (req, res) { res.send(\u0026#34;\u0026lt;h1\u0026gt;Socket IO project folder setup\u0026lt;/h1\u0026gt;\u0026#34;) }) // Calls it with the io object. initListeners(io) http.listen(3000, function () { console.log(\u0026#34;listening on *:3000\u0026#34;) }) And so far, we have just bootstrapped event listeners to events that could be emitted from the client.\nWhen we need to add a new event listener, we just have to define it in a file in the /src/listeners directory, its full path should be: src/listeners/someNewEvent.js, while its basic content would be:\nmodule.exports = function (io) { io.on(\u0026#34;someEventFromClient\u0026#34;, function (socket) { socket.emit(\u0026#34;responseToSomeEventFromClient\u0026#34;, { data: {}, socket }) }) } That is all about listeners; next is for our events.\nWe have seen that in the listeners, we were sending some events back to the client. This might suffice for a tiny application; let\u0026rsquo;s consider a scenario:\nSuppose when a new event is sent to the server, we need to pull the user information from the database, make some adjustments, and send them a new copy of the user data.\nIn the scenario above, it is sufficing enough to have all the user manipulation in the listener for the event, but giving the user the updated information is a task that should be done inside of an event emitter, a major reason being that we can reuse this event emitter and also maintain consistent naming across the codebase (both frontend and backend)\nGetting our hands cleaned away from COVID-19, firstly, let\u0026rsquo;s define a folder like so src/events, and add our index file. The content of the event should look like this:\n\u0026ldquo;`js // src/events/index.js\nmodule.exports = io =\u0026gt; { const fs = require(\u0026ldquo;fs\u0026rdquo;); const path = require(\u0026ldquo;path\u0026rdquo;);\nconst eventsPath = path.resolve(__dirname);\nfs.readdir(eventsPath, (err, files) =\u0026gt; { if (err) { console.error(err); process.exit(1); }\nfiles.map(fileName =\u0026gt; { if (fileName !== \u0026quot;index.js\u0026quot;) { module.exports[fileName] = require(path.resolve(__dirname, fileName)); } }); }); };\nThe code above is similar to what we defined in `src/listeners/index.js`, the difference in this case is that we are exporting all the files in the `src/events` folder aside the `index.js` file. Awesome! Next, let\u0026#39;s define our _connected_ event, which will emit the client once the socket has been connected. So, we will create a new file `src/events/connected.js` and have it look like this ```js const event = (socket) =\u0026gt; { // Do some interesting thing inside of this place! socket.emit(\u0026#34;socker\u0026#34;, {socket, /* Some other interesting data, maybe*/}) } module.exports = event Simple! We have bootstrapped our event files with separation of concerns being considered.\nThe next question, how do we consume events in our listeners with this structure. For this case, let\u0026rsquo;s make a modification to src/listeners/connected.js to be:\nconst { connectedEvent } = require(\u0026#34;../events\u0026#34;) module.exports = function (io) { io.on(\u0026#34;connection\u0026#34;, function (socket) { connectedEvent(socket) }) } First, we import the connectedEvent from the events module, and in line 5, we move changed calling emit to calling the imported event module.\nAwesome, right?\nNext, we can focus on building our application around this ultra-simple architecture and still maintaining it in the future.\nConclusion In this post, we have been able to bootstrap a socketIO application, ensure that we have separation of concerns all through.\nWe could add more improvement by ensuring that the names of the events come from a single file. This can assist us in having consistency and reducing the effort when it is time for documentation.\nSometimes, we will focus a post on ensuring that we persist the socket object after it has been initialized.\nKeep hacking, wash your hands regularly, stay safe, stay at home!\n","permalink":"https://limistah.dev/posts/socketio-app-structure/","summary":"Introduction SocketIO is a JavaScript library that makes developers\u0026rsquo; lives easier when dealing with web socket and socket programming. This is the fact that SocketIO has abstracted out all the low-level and tedious steps that are associated with setting up a socket server and client; it has made the question of programmers be \u0026ldquo;How can I structure my application.\u0026rdquo;\nWhile I have done different types of socket implementations, I will walk us through a setup that has always work for me and has proven to be the best in cases that I have had to use SocketIO.","title":"SocketIO - App structure and architecture"},{"content":"In computation systems, names like concurrent, sequential, parallel, serial, synchronous, asynchronous, non-blocking, shared state, message passing, and likes, stand as a forbearer for the actual task that happens in a system.\nWhile all of the above techniques have their use cases, in the world of JavaScript, asynchronous and synchronous programming never leave the tongues of its programmers.\nIn his Concurrency glossary, slikts (dabas@untu.ms) wrote about asynchronous, he said:\nAsynchrony means \u0026ldquo;not happening at the same time\u0026rdquo;, and asynchronous message passing is a communication model that does not require the sending and receiving to be synchronized, meaning that the sender isn\u0026rsquo;t blocked until the receiver is ready.\nIt might not be clear; still, we will go into a later detail about this with a real-life example. Don\u0026rsquo;t tell anyone, life is mostly asynchronous.\nWhile languages like Java, C#, C++, etc. run their computation on their main thread and could spawn out a new thread to run another set of instructions which are in parallel to the main thread and could also communicate with it. JavaScript, in its uniqueness, does not support that model of computation.\nIn JavaScript, all computations and instructions run in a sequence (i.e., one after another) in a single thread. This means that for instructions ranging from A, B, C, D, E to be executed, A would be executed first and when done, B is then executed, then C, then D, then E, which sees the program to its end.\nStill, with the single-threaded nature of JavaScript, there is a unique feature that makes it outstanding, it is its non-blocking I/O model.\nI/O(Input/Output) could be in any form, like fetching of data over the internet, getting a file from the filesystem and likes, all of these processes does not block the main thread from continuing its execution.\nA real life case study In high school, when students are given an assignment, they are required to submit it. Once they submit the task to the teacher, they are expected to wait until their books are returned back to them already marked/graded.\nWhile they had submitted the assignments and are waiting for the results, they could do other things like reading, attend classes, play games, and joke with friends.\nOnce the teacher is done marking, the students are notified, then they would go to the teacher to pick them up, completing the process.\nIn this scenario, the students have done a task asynchronously. While the teacher could only do one thing at a time, they have asked the teacher to teach them and still mark their assignments.\nThe teacher could not complete both of them at the same time, so they would have to wait, but, set a trigger (through the class rep/governor) that made them know that the assignments are ready and the results are out.\nThe notification could help the students to determine what next to do. Maybe, to proceed with putting their books into their bags or doing the corrections or learning from other classmates. (whatever is next)\nThat is what happens in asynchronous programming.\nAsynchrony in JavaScript JavaScript as a language comes with support for asynchronous programming; there have been different ways to come up with this. Most common is the callbacks, which could be seen in many legacy code bases.\nIn the modern JavaScript engine, there is an added support for writing asynchronous code. We would be exploring these options and their demerits, and I would conclude with why I think that Async/Await as a feature of the language is too essential to understand. Let\u0026rsquo;s hit the start knob.\nCallbacks For seasoned developers, this might seem like unveiling a nightmare following their experiences with callback hell. But, a callback is not that bad.\nA callback is a function passed during a function call/invocation to be executed when the called function determines. A basic example is what setTimeout, setInterval do. Once any of them get called, it takes the function and executes it at the specified time passed from their second argument.\nsetTimeout(() =\u0026gt; console.log(\u0026#34;This is a callback 1 sec. setTimeout test\u0026#34;), 1000) setInterval( () =\u0026gt; console.log(\u0026#34;This is a callback 1 sec. setInterval test\u0026#34;), 1000 ) The two function calls above receive a callback as their first argument and execute it once they each reach the time (1sec) in their second argument. They will never obstruct the main thread from continuing executing other functions; instead, an entry is made in the call stack to take care of this.\nAnother great example is in the DOM Events API. A basic onclick event could be listened to like this:\nconst button = document.getElementById(\u0026#34;#submitButton\u0026#34;) button.addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { console.log(\u0026#34;Submit Button Clicked\u0026#34;) }) Once the button is clicked, with the single thread nature of JavaScript, one would expect that the above code should work like this:\nStore the DOM reference of submitButton to the button variable. button.addEventListener is called which puts in an entry into the call stack The event is registered against a click name, and the code for the addEventListener is executed. Once it is executed, the function/handler should be triggered immediately in the addEventListener execution context As stated in the above, we expect all of those to happen in parallel. But, what happens there is that code and functions are delegated to a later time when an actual action has happened. Recall the High School Assignment illustration we gave?\nWhat happens is that:\nOnce the click event has been triggered on the button, the function would be executed. And this could happen any time in the future, but other code could still run while waiting for this.\nGood Ol\u0026rsquo;days of AJAX With the myriads of NPM libraries that make this happen in a few lines of JS code. Also, the inclusion of the fetch API to the modern JavaScript engines has made this to be so uncommon to the modern developers what AJAX means right in the browser, as they take away the underlying fact about AJAX web requests.\nBefore the days of EcmaScript 6, while developers still really disliked working with JavaScript (which is unavoidable for web developers, though 😄), web requests could be made over the internet through an XMLHTTPRequest object. The technique for making this request was later formed to be called Asynchronous JavaScript and XML, popularly known as AJAX.\nWhat AJAX means is that we could request the internet, wait for the result to come while still having other things going on in the browser (basically, a loading icon). This is asynchronous in nature, and the main reason why it is called AJAX.\nAJAX uses the callback model, a basic vanilla AJAX request using the XMLHTTPRequest object looks like this:\n// Picked from https://www.w3schools.com/xml/xml_http.asp var xhttp = new XMLHttpRequest() xhttp.onreadystatechange = function () { if (this.readyState == 4 \u0026amp;\u0026amp; this.status == 200) { // Typical action to be performed when the document is ready: document.getElementById(\u0026#34;demo\u0026#34;).innerHTML = xhttp.responseText } } xhttp.open(\u0026#34;GET\u0026#34;, \u0026#34;filename\u0026#34;, true) xhttp.send() It is not attractive, I know. If you are a modern JavaScript developer, respect those that have passed through this and still look uncool to you, this was so cool to them back then.\nThis AJAX feature utilized the model of event handling and was very easy to comprehend by JavaScript developers. But, it shares a similar problem with all callback-based coding patterns/techniques.\nCallback Hell Does it mean hell in our code/machines, well, not necessarily? It is something exciting to know some of its details.\nImage source\nLet\u0026rsquo;s tell of a story of a codebase that illustrates this.\nFirst let\u0026rsquo;s define a function that takes a callback. It would add two numbers together, then pass the result to a function defined as the third argument, clear enough? I believe!\nconst sumTwoNumbers = (a, b, doneCB) =\u0026gt; doneCB(a + b) Next, let\u0026rsquo;s determine an Array of numbers but try\nconst numbers = Array(1, 2, 3, 4, 5) Now, we can add up the numbers successively using Array.prototype.forEach:\nconst totalAdditions = 0 numbers.forEach((number, index) =\u0026gt; { sumTwoNumbers(number, numbers[index + 1], (total) =\u0026gt; { totalAdditions += total }) }) As seen above, we are just two levels deep into the callback calls, and it is not getting any interesting. It should be noted that as the callback levels increase, the risk of hitting a re-initialization of variables increases. The first callback might have initialized numbers re-initializing it might disrupt the logic of the code, who knows?\nAnd looking at an example from the below image, we can see that it can get so uninteresting in larger applications.\nPromises to the rescue Eric Elliott: A promise is an object that may produce a single value some time in the future: either a resolved value, or a reason that it’s not resolved (e.g., a network error occurred).\nIn JavaScript, Promises have made the life of programmers more easier in handling asynchronous code. With Promises, most of the underlying problems like callback hells are waved out.\nPromises has a very interesting background in JavaScript, Eric Elliott gave has a great information about promises, regarding that, a promise could be constructed in a few lines of code:\nlet promise = new Promise(function (resolve, reject) { // executor }) In constructing a promise, we call the Promise constructor with a function called the executor. This function(executor) receives two arguments labeled as resolve and reject, both of which are functions.\nThe resolve argument is expected to be called with a value resolve(value) only when the executor has finished doing its tasks without any error. While the reject is expected to be called with a value (usually of type Error) reject(error) only if the promise failed or faced an error.\nlet promise = new Promise(function (resolve, reject) { try { const value = 2 * 2 resolve(value) } catch (e) { reject(e) } }) There are three (3) notable methods for every Promise object.\n.then() Accepts a function that receives the value passed when resolve is called by the executor. Only get called when the executor calls resolve. .catch() Accepts a function that receives the value passed when reject is called by the executor. Only get called when the executor calls reject. .finaly() The object returned by every initialization of a Promise constructor has some special attributes and methods. One of this is the status promise.status property, which tells about the current state of the Promise.\nThe status properties has three (3) possible values:\nPending: This is a state before resolve() is called. Resolved: This is a state after resolve() has been called. Rejected: This is a state after reject() has been called, or an error is thrown in the executing function. Promise are chainable, hence, every .then() called on a promise object returns another promise.\nconst promise = new Promise(function (resolve, reject) { try { const value = 2 * 2 resolve(value) } catch (e) { reject(e) } }) // Consuming the promise promise .then((value) =\u0026gt; value * 4) .then((value) =\u0026gt; value * 6) .then((value) =\u0026gt; console.log(value)) While consuming the promise, every resolver in the .then() function returns a value until the last call to .then(). Failure to do this as we have done in the last call of .then() would make the next call to .then() receive an undefined.\n// Consuming the promise promise .then((value) =\u0026gt; value * 4) .then((value) =\u0026gt; value * 6) .then((value) =\u0026gt; console.log(value)) .then((value) =\u0026gt; console.log(value) /* -\u0026gt; undefined */) We can use the finally in many different ways since it is called irrespective of the state of the Promise. The most obvious case of using it is when we need to hide a loading icon to show a message for the result of the action that has just been carried out.\npromise .finally(() =\u0026gt; { document.querySelector(\u0026#34;.loading-icon\u0026#34;).remove() }) .then((data) =\u0026gt; { // consume data }) .catch((err) =\u0026gt; { // Show error }) There is more to Promises, this only gives a preamble and brief introduction to it, a thorough explanation could be found here\nAsync / Await It is not surprising to note that, for some unknown reasons that are best known to developers, they want Promises to be synchronous.\nThey found themselves in a situation where they need the result of a Promise to be available before they proceed to the next chunk of code - Either it is a network request, or a result from an extensive math function, they just needed the result to be available right in the next line of their code.\nTo solve this for them, the folks at TC39 gave us Async/Await.\nThis feature assists in constructing a promise right from a function, and we could wait for the result of a Promise before moving to the next line of code.\nconst getNumber = () =\u0026gt; { return async () =\u0026gt; { return 20 } } const result = await getNumber() Looking closely at the code above, we could see that the getNumber function returns an async function(a Promise), but why not make the returned function the primary function for getNumber?\nNo, we can not. JavaScript does not support making a global function an async function; the best we can do is wrap it in a containing function and make the inner function returns an async function. If we need a global function to be a Promise, it is best that we use a Promise constructor to define it.\nWhen we called const result = await getNumber(), we are telling the engine not to run any other code until we have the result from the Promise returned by getNumber, in this case, 20. If we omit await from that expression, following code will run while running the Promise returned by getNumber in the background.\nIn my honest opinion, I see async as syntactic sugar to constructing a Promise on the fly. If we would want to attempt to have the getNumber as a promise constructor instead of an async/await we would have:\nconst getNumber = new Promise(function (resolve, reject) { try { setTimeout(() =\u0026gt; resolve(20), 60 * 1000) } catch (e) { reject(e) } }) In the above code, we have waited for about 1minute for the promise to be resolved if at all no error is thrown in our executor.\nDo you want to learn more about Async/Await and be a Ninja of asynchronous javascript? Try seeing the MDN guide on the topic.\nConclusion As seen above, we can deduce that the underlying engine of JavaScript supports asynchronous coding. At the same time, this is great; it created some problems for developers trying to make their code asynchronous.\nAs a solution to the challenge, Promise was introduced into the language, which was followed by the inclusion of Async/Await construct.\nIn my honest opinion, writing Async/Await in my codebases has made my code not just readable, but very predictable.\nSo, to asynchronous Javascript coding, I highly recommend understanding Promises and Async/Await construct if at all we want our future self and other developers to say a prayer when they see our code after we write it.\nShalom!\n","permalink":"https://limistah.dev/posts/asynchronous-javascript/","summary":"In computation systems, names like concurrent, sequential, parallel, serial, synchronous, asynchronous, non-blocking, shared state, message passing, and likes, stand as a forbearer for the actual task that happens in a system.\nWhile all of the above techniques have their use cases, in the world of JavaScript, asynchronous and synchronous programming never leave the tongues of its programmers.\nIn his Concurrency glossary, slikts (dabas@untu.ms) wrote about asynchronous, he said:\nAsynchrony means \u0026ldquo;not happening at the same time\u0026rdquo;, and asynchronous message passing is a communication model that does not require the sending and receiving to be synchronized, meaning that the sender isn\u0026rsquo;t blocked until the receiver is ready.","title":"Asynchronous Javascript"},{"content":"You might have been in this kind of trap before or currently in one, well, I just want to tell you that I know your pain.\nIn a custom built CMS, managing of Menu and navigation in the site from the Admin Dashboard is a requirement. While it is interesting to use, it is not as interesting to build.\nI just walked past this process, here, I am sharing how I have conquered it.\nTo consider I have a limitation for the menu to be deeply nested not more than two steps, so in this case, this is to ensure that the loops that I will write won\u0026rsquo;t have to be too recursive - which if not properly handled could create havoc.\nModel/Database In my database, I have the menu store as a flat document with each of them having their own information. The menu model has the following properties:\nposition: Position of the menu (Determined by the application, in my case, \u0026ldquo;Main Nav\u0026rdquo;, and \u0026ldquo;Footer Menu\u0026rdquo;) **name:**Text that the user would see parent: String containing the index of the parent or index of the grand parent with the index of the parent separated by a dot in the case of nesting url: The URL to map the navigation to weight The sorting order for the menu. So, with the above fields, I can create a menu like this:\nconst mainMenu = [ { name: \u0026#34;Home\u0026#34;, url: \u0026#34;/\u0026#34;, position: \u0026#34;Main Nav\u0026#34;, parent: null, weight: 1 }, { name: \u0026#34;Listings\u0026#34;, url: \u0026#34;/listings\u0026#34;, position: \u0026#34;Main Nav\u0026#34;, parent: null, weight: 2 }, { name: \u0026#34;Categories\u0026#34;, url: \u0026#34;/categories\u0026#34;, position: \u0026#34;Main Nav\u0026#34;, parent: null, weight: 3 }, { name: \u0026#34;Pricing\u0026#34;, url: \u0026#34;/pricing\u0026#34;, position: \u0026#34;Main Nav\u0026#34;, parent: null, weight: 4 }, { name: \u0026#34;Pages\u0026#34;, url: \u0026#34;#\u0026#34;, position: \u0026#34;Main Nav\u0026#34;, parent: null, weight: 5 }, { name: \u0026#34;About\u0026#34;, url: \u0026#34;#\u0026#34;, position: \u0026#34;Main Nav\u0026#34;, parent: 5, weight: 1 }, { name: \u0026#34;Contact\u0026#34;, url: \u0026#34;/contact\u0026#34;, position: \u0026#34;Main Nav\u0026#34;, parent: \u0026#34;5.1\u0026#34;, weight: 1 } ].map(async menu =\u0026gt; { const mainMenu = new Menu(); Object.assign(mainMenu, menu); // Populates the fields to the mainMenu object await mainMenu.save(); // Saves the menu }); First problem has been solved.\nNext problem is making the data of the menu available in a format that we can render more easily.\nMenu parser middleware To solve this, I created a middleware which sets a global value for my view engine. The code in the middleware looks like this:\nasync handle({ request, view }, next) { // call next to advance the request // Can be MongoDB Model, this is AdonisJS const MenuService = use(\u0026#34;App/Services/Admin/MenuService\u0026#34;); const menuService = new MenuService(); const menus = await menuService.findAll(\u0026#34;position\u0026#34;); // Parsing of the menu comes in here // At this point built menu contains keys of menu positions, the value of every keys is an array of nested values that can be passed to drag and drop UI libraries like JQuery nestable for admin configuration using a browser. // Adonis code to share values across all views. view.share({ siteMenus: builtMenu }); } Parsing of Zero Level menus and sorting After the middleware has been defined, first is to understand that our menu won\u0026rsquo;t go more than two levels, with the zero level being the parent to other menus. To get the parent out of the menu, we map through all the items in the menus variable, getting all the elements that has just no dot in their parent value then storing them at a position defined in the builtMenu object\nconst builtMenu = {}; const menuWithParent = []; // Stores all menu that has parent for later processing // First of all, gather all the menus with same position to the same index menus.map(menu =\u0026gt; { // The position for the current menu does not exist in the builtMenu object if (!builtMenu[menu.position]) { // Create it with an empty array builtMenu[menu.position] = []; } // If this menu does not have parent, push it directly into its position key in the builtMenu variable if (!menu.parent) { builtMenu[menu.position].push(menu); } else { // Else, push for later processing menuWithParent.push(menu); } }); Before we move forward, we should add code that sorts the generated menu so far, the easiest way is to get the keys in the built menu and map through each keys, then running the sort aggregation method on each item of the builtMenu data.\nThis sorting only compares the weight of each menu item, these weights are expected to go in ascending order, hence we can use a.weight \u0026gt; b.weight so that we can have lowest number comes first ahead of larger number\nconst sortedMenu = {}; // Holds all the menu that has been sorted // Sorts all the menu in their respective positions, just to maintain ordering Object.keys(builtMenu).map(positionKey =\u0026gt; { const menuAtPosition = builtMenu[positionKey]; sortedMenu[positionKey] = menuAtPosition.sort( (a, b) =\u0026gt; a.weight \u0026gt; b.weight ); }); At this stage, our builtMenu should look like this:\n{ \u0026#34;Main Menu\u0026#34;: [ { ...menu // \u0026lt;- Menu details }, { ...menu // \u0026lt;- Menu details }, { ...menu // \u0026lt;- Menu details }, { ...menu // \u0026lt;- Menu details }, { ...menu, // \u0026lt;- Menu details } ] } What we have to do next is work with the menus that exist in the menuWithParent variable that was gathered while pulling the menus that do not have child/grandchild.\nParsing of First Level menus and sorting To do this, we map through all the item in the variable, ensure that it does not have grandparent, if it does we push it to the menuWithGrandParent array for later processing, if it does not, we append the menu to the parent after creating a children array in to the parent if children array does not exist in the parent.\nWe also sort the menu generated so far after pushing to the parent, to maintain ordering of the menu.\n// This variable holds the menus that has grandparent const menuWithGrandParent = []; // We want to work on the menu that has parent menuWithParent.map(menu =\u0026gt; { // following the structure of our menu in the database, parentIndex is the value of the parent key of every menu that has parent let parentIndex = menu.parent; // We still have to determine if the menu is a grandchild of a parent, so we split by the dot separator that we have used then use the length of the array as a determinant const hasGrandChild = parentIndex.split(\u0026#34;.\u0026#34;).length \u0026gt; 1; // This menu is not a grandchild, we can proceed with our process if (!hasGrandChild) { // We need to get the position of the menu const menusAtPosition = builtMenu[menu.position]; parentIndex = parseInt(parentIndex); // Decrements the parentIndex to get the index in the array, in any case the reducing by 1 is lesser than zero, we want to default to index 0 else we want to get the index in the array const parent = menusAtPosition[parentIndex - 1 \u0026lt; 0 ? 0 : parentIndex - 1]; // If the parent does not have children, we to initialize that to an array if (!parent.children) { parent.children = []; } // Push the new child of the parent into its children parent.children.push(menu); // Sort the children in the parent to maintain ordering parent.children.sort((a, b) =\u0026gt; a.weight \u0026gt; b.weight); } else { // This menu has a grandparent, we can not process it like this, we push to the menuWithGrandParent array and process after this menuWithGrandParent.push(menu); } }); And our builtMenu should now look like this:\nAt this stage, our builtMenu should look like this:\n{ \u0026#34;Main Menu\u0026#34;: [ { ...menu // \u0026lt;- Menu details }, { ...menu // \u0026lt;- Menu details }, { ...menu // \u0026lt;- Menu details }, { ...menu // \u0026lt;- Menu details }, { // Parent Menu ...menu, // \u0026lt;- Menu details children: [ { // Child Menu ...menu, // \u0026lt;- Menu details }, ] } ] } Parsing of Second Level menus and sorting Now that we have the parent and children out, we are left with just one level deep, which is for the grandchild. The code is similar to the one that generate the children menus just that we have to get the index for the parent, grandparent and ensure that they have children or we create one for them before we can append the new array into them.\n// Processes menu that has grandparent menuWithGrandParent.map(menu =\u0026gt; { // Get the parent of the parent let menuParent = menu.parent; // Split the parent to get the index of the parent and grand parent const indexSplit = menuParent.split(\u0026#34;.\u0026#34;); // Get the position of this menu const grandChildMenuPosition = builtMenu[menu.position]; // Get the index of the grand parent const grandParentIndex = indexSplit[0] - 1 \u0026lt; 0 ? 0 : indexSplit[0] - 1; // Set the index of the parent const parentIndex = indexSplit[1] - 1 \u0026lt; 0 ? 0 : indexSplit[1] - 1; // The menus at the position of the grand child menu using the grand parent index const grandParentMenu = grandChildMenuPosition[grandParentIndex]; // If this grand parent has children, which we expect that it should if (grandParentMenu.children) { // We get the parent of the menu from the grandparent\u0026#39;s children const parent = grandParentMenu.children[parentIndex]; // if the parent exists if (parent) { // Initialize children on the parent if it has none if (!parent.children) { parent.children = []; } // Push the menu to the parent under the grand parent parent.children.push(menu); // SOrt the children of the parent of the grandchild to maintain the ordering parent.children.sort((a, b) =\u0026gt; a.weight \u0026gt; b.weight); } } }); Finally, our builtMenu should look like this:\n{ \u0026#34;Main Menu\u0026#34;: [ { ...menu // \u0026lt;- Menu details }, { ...menu // \u0026lt;- Menu details }, { ...menu // \u0026lt;- Menu details }, { ...menu // \u0026lt;- Menu details }, { // Grand Parent Menu ...menu, // \u0026lt;- Menu details children: [ { // Parent Menu ...menu, // \u0026lt;- Menu details ...menu, // \u0026lt;- Menu details children: [ { // Child Menu ...menu, // \u0026lt;- Menu details }, ] }, ] } ] } Putting it all together Compiling all together, the middleware looks like:\nasync handle({ request, view }, next) { // call next to advance the request // Can be MongoDB Model, this is AdonisJS const MenuService = use(\u0026#34;App/Services/Admin/MenuService\u0026#34;); const menuService = new MenuService(); const menus = await menuService.findAll(\u0026#34;position\u0026#34;); const builtMenu = {}; const menuWithParent = []; // Stores all menu that has parent for later processing // First of all, gather all the menus with same position to the same index menus.map(menu =\u0026gt; { // The position for the current menu does not exist in the builtMenu object if (!builtMenu[menu.position]) { // Create it with an empty array builtMenu[menu.position] = []; } // If this menu does not have parent, push it directly into its position key in the builtMenu variable if (!menu.parent) { builtMenu[menu.position].push(menu); } else { // Else, push for later processing menuWithParent.push(menu); } }); const sortedMenu = {}; // Holds all the menu that has been sorted // Sorts all the menu in their respective positions, just to maintain ordering Object.keys(builtMenu).map(positionKey =\u0026gt; { const menuAtPosition = builtMenu[positionKey]; sortedMenu[positionKey] = menuAtPosition.sort( (a, b) =\u0026gt; a.weight \u0026gt; b.weight ); }); // This variable holds the menus that has grandparent const menuWithGrandParent = []; // We want to work on the menu that has parent menuWithParent.map(menu =\u0026gt; { // following the structure of our menu in the database, parentIndex is the value of the parent key of every menu that has parent let parentIndex = menu.parent; // We still have to determine if the menu is a grandchild of a parent, so we split by the dot separator that we have used then use the length of the array as a determinant const hasGrandChild = parentIndex.split(\u0026#34;.\u0026#34;).length \u0026gt; 1; // This menu is not a grandchild, we can proceed with our process if (!hasGrandChild) { // We need to get the position of the menu const menusAtPosition = builtMenu[menu.position]; parentIndex = parseInt(parentIndex); // Decrements the parentIndex to get the index in the array, in any case the reducing by 1 is lesser than zero, we want to default to index 0 else we want to get the index in the array const parent = menusAtPosition[parentIndex - 1 \u0026lt; 0 ? 0 : parentIndex - 1]; // If the parent does not have children, we to initialize that to an array if (!parent.children) { parent.children = []; } // Push the new child of the parent into its children parent.children.push(menu); // Sort the children in the parent to maintain ordering parent.children.sort((a, b) =\u0026gt; a.weight \u0026gt; b.weight); } else { // This menu has a grandparent, we can not process it like this, we push to the menuWithGrandParent array and process after this menuWithGrandParent.push(menu); } }); // Processes menu that has grandparent menuWithGrandParent.map(menu =\u0026gt; { // Get the parent of the parent let menuParent = menu.parent; // Split the parent to get the index of the parent and grand parent const indexSplit = menuParent.split(\u0026#34;.\u0026#34;); // Get the position of this menu const grandChildMenuPosition = builtMenu[menu.position]; // Get the index of the grand parent const grandParentIndex = indexSplit[0] - 1 \u0026lt; 0 ? 0 : indexSplit[0] - 1; // Set the index of the parent const parentIndex = indexSplit[1] - 1 \u0026lt; 0 ? 0 : indexSplit[1] - 1; // The menus at the position of the grand child menu using the grand parent index const grandParentMenu = grandChildMenuPosition[grandParentIndex]; // If this grand parent has children, which we expect that it should if (grandParentMenu.children) { // We get the parent of the menu from the grandparent\u0026#39;s children const parent = grandParentMenu.children[parentIndex]; // if the parent exists if (parent) { // Initialize children on the parent if it has none if (!parent.children) { parent.children = []; } // Push the menu to the parent under the grand parent parent.children.push(menu); // SOrt the children of the parent of the grandchild to maintain the ordering parent.children.sort((a, b) =\u0026gt; a.weight \u0026gt; b.weight); } } }); // At this point built menu contains keys of menu positions, the value of every keys is an array of nested values that can be passed to drag and drop UI libraries like JQuery nestable for admin configuration using a browser. // Adonis code to share values across all views. view.share({ siteMenus: builtMenu }); } Consuming the menu data from frontend The easiest way to consume the menu is to use a JQuery plugin called nestable++, the plugin make updating, creating and storing of the menu easier with the data that we have.\nIt expects an ordered list to be populated with list items and if children is expected, the list items should contain another unordered list with the children as the list items.\nIn our own case, once the view has set a JavaScript object with the menu data as its value, we can do something like this:\nlet CURRENT_MENU = \u0026#34;Main Menu\u0026#34;; $(function() { function decodeHtml(html) { var txt = document.createElement(\u0026#34;textarea\u0026#34;); txt.innerHTML = html; return txt.value; } const siteMenus = JSON.parse(decodeHtml(window._siteMenus)); // This initializes the drag and drop nestable listing nestableMenuBuilder(siteMenus[CURRENT_MENU], { maxDepth: 3 }); }); The nestableMenuBuilder function looks like this:\nconst nestableMenuBuilder = function(menuList, options = {}) { // Creates a single menu item that the nestable understands const menuItem = function(item) { return $(` \u0026lt;li class=\u0026#34;dd-item\u0026#34; data-id=\u0026#34;${item.id}\u0026#34; data-name=\u0026#34;${ item.name }\u0026#34; data-slug=\u0026#34;${item.url}\u0026#34; data-new=\u0026#34;${item.new || 0}\u0026#34; data-deleted=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;dd-handle\u0026#34;\u0026gt;${item.name}\u0026lt;/div\u0026gt; \u0026lt;span class=\u0026#34;button-delete btn btn-default btn-xs pull-right\u0026#34; data-owner-id=\u0026#34;${item.id}\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;nestable-icon\u0026#34; data-feather=\u0026#34;x\u0026#34; \u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;button-edit btn btn-default btn-xs pull-right\u0026#34; data-owner-id=\u0026#34;${item.id}\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;nestable-icon\u0026#34; data-feather=\u0026#34;edit-3\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/li\u0026gt;`); }; const buildNestable = function(items) { const $parent = $(\u0026#39;\u0026lt;ol class=\u0026#34;dd-list\u0026#34;\u0026gt;\u0026lt;/ul\u0026gt;\u0026#39;); const createChildren = function(items, $_parent) { return items.map(item =\u0026gt; { const $_el = menuItem(item); if (item.children) { const $_parent = $(\u0026#39;\u0026lt;ol class=\u0026#34;dd-list\u0026#34;\u0026gt;\u0026lt;/ul\u0026gt;\u0026#39;); createChildren(item.children, $_parent); $_el.append($_parent); } $_parent.append($_el); }); }; items.map(item =\u0026gt; { const $_el = menuItem(item); if (item.children) { const $_parent = $(\u0026#39;\u0026lt;ol class=\u0026#34;dd-list\u0026#34;\u0026gt;\u0026lt;/ul\u0026gt;\u0026#39;); createChildren(item.children, $_parent); $_el.append($_parent); } $parent.append($_el); }); return $parent; }; const nestableList = buildNestable(menuList); // Initializes nestable++ $(\u0026#34;.dd.nestable\u0026#34;) .empty() .append(nestableList); }; In our html, we have to include the nestable++ plugin from here and its css from here and JQuery, then create an ol with class names dd nestable.\n\u0026lt;ol class=\u0026#34;dd nestable\u0026#34;\u0026gt;\u0026lt;/ol\u0026gt; And, that is it.\nConclusion There are other features of nestable that I do not include here, like updating, adding new item and deleting a menu item. I feel those are for nestable++ itself, including them here would take us out of scope for the post.\n","permalink":"https://limistah.dev/posts/wordpress-like-menu-node-apps/","summary":"You might have been in this kind of trap before or currently in one, well, I just want to tell you that I know your pain.\nIn a custom built CMS, managing of Menu and navigation in the site from the Admin Dashboard is a requirement. While it is interesting to use, it is not as interesting to build.\nI just walked past this process, here, I am sharing how I have conquered it.","title":"Menu system in Node Apps like WordPress Menu"},{"content":"Beginners in ReactJS often face this kind of error: It is not just with inputs, it is with all HTML elements that does not expect a closing tags, they are called empty elements. A list of these tags could be found here.\nTo solve this very easily, we just have to follow the HTML semantics by ending all empty elements with /\u0026gt; instead of \u0026gt;\nSo inputs should look like this:\n\u0026lt;input type=\u0026#34;text\u0026#34; /\u0026gt; For image\n\u0026lt;img href=\u0026#34;text\u0026#34; alt=\u0026#34;profile avatar\u0026#34; /\u0026gt; Just note that:\nIn HTML the ending slash in optional, in ReactJS applications, it is required\nAnd likes\u0026hellip;.\nI hope this saves someone some nagging and head nuts.\n","permalink":"https://limistah.dev/posts/react-unexpected-closing-tag/","summary":"Beginners in ReactJS often face this kind of error: It is not just with inputs, it is with all HTML elements that does not expect a closing tags, they are called empty elements. A list of these tags could be found here.\nTo solve this very easily, we just have to follow the HTML semantics by ending all empty elements with /\u0026gt; instead of \u0026gt;\nSo inputs should look like this:","title":"ReactJS - Unexpected closing tag"},{"content":"Last time, I was trying to render a data table and I thought that I should make some fields editable right in the table listing. It is interesting to note that I never thought about any NPM library for it, so I went all out to create a simple but effective solution for myself.\nIn this post, I will demonstrate how I created an editable component. The component would be able to use different form fields and notify the parent if any change has been made. That said, Let\u0026rsquo;s move\u0026hellip;.\nTo start with, I will define the usage that I had for the component. The component to be was consumed like this:\n\u0026lt;InplaceEdit isUpdatingId={true || false} id={idForTheField || \u0026#34;\u0026#34;} updateKey={keyToReturnAnObjectWith || \u0026#34;\u0026#34;} onUpdate={functionToCallForTheUpdate || () =\u0026gt; {}} text={textForDisplay || \u0026#34;\u0026#34;} defaultValue={defaultInCaseValueDoesNotMatch || \u0026#34;\u0026#34;} inputType={htmlInputType || \u0026#34;text\u0026#34;} selectOptions={ optionForSelectInputType || [ { value: true, label: \u0026#34;Yes\u0026#34; }, { value: false, label: \u0026#34;No\u0026#34; } ]} valueToText={ functionToConvertFieldValueToText || value =\u0026gt; (value === \u0026#34;true\u0026#34; ? \u0026#34;Yes\u0026#34; : \u0026#34;No\u0026#34;)} /\u0026gt;; With this, I was able to use the InplaceEdit component almost anywhere that I wanted as far as single value was expected.\nNow, building for the usage. Firstly, I defined the InplaceEdit component:\nimport React from \u0026#34;react\u0026#34; // I won\u0026#39;t repeat this after now function InplaceEdit() { return ( \u0026lt;div\u0026gt; Inplace Edit Component \u0026lt;/div\u0026gt; ); } export default InplaceEdit; // I won\u0026#39;t repeat this after now Next, I wanted to toggle between two modes. One being a normal text, and the other when it is a form field. To do this, I added an onClick handler to the root div element, which updates a state that determines these modes.\nfunction InplaceEdit() { const [showEditor, updateShowEditor] = React.useState(false); const handleShowEditor = () =\u0026gt; { updateShowEditor(true); }; return ( \u0026lt;div onClick={handleShowEditor} style={{ cursor: \u0026#34;pointer\u0026#34; }}\u0026gt; {showEditor ? \u0026#34;Show Editor\u0026#34; : \u0026#34;Show Text\u0026#34;} \u0026lt;/div\u0026gt; ); } Next, I required two distinct components for both the editor and display, I called them EditDisplay and TextDisplay, respectively. This made the InplaceEdit component looked more like this:\nfunction InplaceEdit() { const [showEditor, updateShowEditor] = React.useState(false); const handleShowEditor = () =\u0026gt; { updateShowEditor(true); }; return ( \u0026lt;div onClick={handleShowEditor} style={{ cursor: \u0026#34;pointer\u0026#34; }}\u0026gt; {showEditor ? \u0026lt;EditDisplay/\u0026gt; : \u0026lt;TextDisplay /\u0026gt;} \u0026lt;/div\u0026gt; ); } After that, I defined the EditDisplay component and TextDisplay components:\n// EditDisplay Component function EditDisplay() { return ( \u0026lt;\u0026gt;\u0026lt;/\u0026gt; ); } // Text display component const TextDisplay = () =\u0026gt; \u0026lt;\u0026gt;\u0026lt;/\u0026gt;; So, at this point, I was able to toggle between two modes, but nothing happens and nothing is rendered, yet. To move forward, I had to display the text which would be received by the InplaceEdit component and passed to the TextDisplay component as props. The below code is what I implemented for this phase:\n// EditDisplay Component function EditDisplay() { return ( \u0026lt;\u0026gt;\u0026lt;/\u0026gt; ); } // Text display component const TextDisplay = ({text}) =\u0026gt; \u0026lt;span\u0026gt;{text}\u0026lt;/span\u0026gt;; function InplaceEdit({ text }) { const [showEditor, updateShowEditor] = React.useState(false); const handleShowEditor = () =\u0026gt; { updateShowEditor(true); }; return ( \u0026lt;div onClick={handleShowEditor} style={{ cursor: \u0026#34;pointer\u0026#34; }}\u0026gt; {showEditor ? \u0026lt;EditDisplay/\u0026gt; : \u0026lt;TextDisplay text={text} /\u0026gt;} \u0026lt;/div\u0026gt; ); } I was glad when I got to this point. I am sure you are as well, at least, something is getting rendered.\nIt is very correct if you guessed that the TextDisplay component will not change its state afterwards, and bulk of the work is around the EditDisplay component. So, sorry, I won\u0026rsquo;t be adding that style that you are thinking.\nThe next move I made was to tell the EditDisplay what type of edit is required and the default value for the edit.\n// EditDisplay Component function EditDisplay({ defaultValue, inputType }) { return \u0026lt;\u0026gt;\u0026lt;/\u0026gt; } // Text display component const TextDisplay = ({ text }) =\u0026gt; \u0026lt;span\u0026gt;{text}\u0026lt;/span\u0026gt; function InplaceEdit({ text, inputType, defaultValue }) { const [showEditor, updateShowEditor] = React.useState(false) const handleShowEditor = () =\u0026gt; { updateShowEditor(true) } return ( \u0026lt;div onClick={handleShowEditor} style={{ cursor: \u0026#34;pointer\u0026#34; }}\u0026gt; {showEditor ? ( \u0026lt;EditDisplay inputType={inputType || \u0026#34;text\u0026#34;} defaultValue={defaultValue} /\u0026gt; ) : ( \u0026lt;TextDisplay text={text} /\u0026gt; )} \u0026lt;/div\u0026gt; ) } The defaultValue and inputType are both passed from the InplaceEdit component and passed down to the EditDisplay component, the inputType is short-circuited to return text in case the defaultValue is not set. Not that bad, right?\nThe next step is that, I put up some input components for the update. I had TextInput which looked like this:\nconst TextInput = ({ type, defaultValue, onBlur, disabled }) =\u0026gt; { const handleKeyUp = e =\u0026gt; { if (e.keyCode === 13) { onBlur(e); } }; const inputRef = React.useRef(); React.useEffect(() =\u0026gt; { inputRef.current.focus(); }, [inputRef]); return ( \u0026lt;input ref={inputRef} type={type} defaultValue={defaultValue} onBlur={onBlur} disabled={disabled} onKeyUp={handleKeyUp} /\u0026gt; ); }; This componentTextInput does a pretty interesting stuff, first, it handles the KeyUp event which sends information up to the parent(InplaceEdit) component, it can be disabled, the type is set as from the parent and the defaultValue is also configurable through the props.\nAnd the SelectInput, I defined it to look like this:\nconst SelectInput = ({ options, defaultValue, onBlur, disabled }) =\u0026gt; { return ( \u0026lt;select onChange={onBlur} defaultValue={defaultValue} disabled={disabled}\u0026gt; {options.map(option =\u0026gt; { return \u0026lt;option value={option.value}\u0026gt;{option.label}\u0026lt;/option\u0026gt;; })} \u0026lt;/select\u0026gt; ); }; This component(SelectInput) accepts the options as an array of object with label and value keys, defaultValue for persistence, onBlur event handler and can be disabled by setting its disabled props. Not too serious, I think.\nFor the EditDisplay, I had to render one of the two types of components based on the inputType props:\nconst EditDisplay = ({ inputType, onchange, defaultValue, selectOptions, onBlur, disabled }) =\u0026gt; { return ( \u0026lt;\u0026gt; {inputType === \u0026#34;select\u0026#34; \u0026amp;\u0026amp; ( \u0026lt;SelectInput options={selectOptions} defaultValue={defaultValue} onchange={onchange} onBlur={onBlur} disabled={disabled} /\u0026gt; )} {[\u0026#34;text\u0026#34;, \u0026#34;password\u0026#34;].includes(inputType) \u0026amp;\u0026amp; ( \u0026lt;TextInput type={inputType} onchange={onchange} /\u0026gt; )} \u0026lt;/\u0026gt; ); }; Next, I thought, the EditDisplay component receives some props that are not yet defined in the InplaceEdit component, so I defined them like so:\nfunction InplaceEdit({ text, inputType, defaultValue, valueToText, updateKey, onUpdate, }) { const [showEditor, updateShowEditor] = React.useState(false) // Keeps the text synchronized const [displayText, updateDisplayText] = React.useState(text) const handleShowEditor = () =\u0026gt; { updateShowEditor(true) } // Callback for the onBlur event handler const handleEditorInputChange = React.useCallback(e =\u0026gt; { const value = e.currentTarget.value // Converts the value to a displayable text using a function from the parent consumer const text = typeof valueToText === \u0026#34;function\u0026#34; ? valueToText(value) : value // Updates the display text for synchronization updateDisplayText(text) // Hides the editor updateShowEditor(false) if (defaultValue !== value) { // If the value has changed, we need to tell the consumer parents about it const handler = typeof onUpdate === \u0026#34;function\u0026#34; ? onUpdate : () =\u0026gt; {} // Sets the data returning an object containing the value in the updateKey or just the value itself const data = updateKey ? { [updateKey]: value } : value // Call the update function with the data and resource id handler(data, id) } }) return ( \u0026lt;\u0026gt; \u0026lt;div onClick={handleShowEditor} style={{ cursor: \u0026#34;pointer\u0026#34; }}\u0026gt; {showEditor ? ( \u0026lt;EditDisplay inputType={inputType || \u0026#34;text\u0026#34;} defaultValue={defaultValue} onBlur={handleEditorInputChange} selectOptions={selectOptions} disabled={isUpdatingId === id} /\u0026gt; ) : ( \u0026lt;TextDisplay text={displayText} /\u0026gt; )} \u0026lt;/div\u0026gt; \u0026lt;/\u0026gt; ) } Noticeable from the above snippet is that, the disabled prop is only set if the isUpdatingId is equals to the id props, and the \u0026lt;TextDisplay /\u0026gt; now receives a displayText state which is synchronized across the two children of InplaceEdit component.\nAnd guess what, that is all there is to an InplaceEdit component that returns just a single value. It should accept a value/text and toggles between input and text states, too damn simple, I think.\nConclusion This was a very interesting process for me, and I never regret it and never will. There are awesome React components for this and does better job than my local component. But I thougth, I should experiment a bit, and here I have it.\nThe full code is available as a gist here, if you want to check it out. And, thank you for reading.\nDhanyavaad! 🙇\n","permalink":"https://limistah.dev/posts/reactjs-inplace-edit-component/","summary":"Last time, I was trying to render a data table and I thought that I should make some fields editable right in the table listing. It is interesting to note that I never thought about any NPM library for it, so I went all out to create a simple but effective solution for myself.\nIn this post, I will demonstrate how I created an editable component. The component would be able to use different form fields and notify the parent if any change has been made.","title":"ReactJS - Inplace Edit component"},{"content":"Modern application development requires that some actions are carried out when a point of the application is reached.\nTasks like confirmation email, invoice generation, logging and profiling are few of things that requires to be carried out in specific regions of application flow.\nThese actions that are triggered are called Events. Events in modern application development make code execution after a web request to the server has been completed to be possible.\nIn this post, I will demonstrate how to implement events in AdonisJS.\nFirstly, from the terminal, in an AdonisJS application directory, execute this command:\nadonis make: listener NewUser This should show an output similar to:\n✔ create app/Listeners/NewUser.js Indicating that a file has been created at app/Listeners/NewUser.js. Right!\nNext, we need to define what happens when this event is called. We would go into the newly generated file and write some code into it. In my case, I would just log he details of the new user.\n\u0026#39;use strict\u0026#39; const NewUser = exports = module.exports = {} NewUser.registered = async (user) =\u0026gt; { console.log(user) } At this point, we need to move forward to tell AdonisJS about our listener. We can do this by going to start/events.js, sometimes the file does not exist, you are free to create yourself.\nInside of start/events.js I have:\nconst Event = use(\u0026#34;Event\u0026#34;); Event.on(\u0026#34;NewUser::registered\u0026#34;, \u0026#34;NewUser.registered\u0026#34;); The above code loads the Event from the IoC, then I called the .on method to register an event named NewUser::registered, then determines the listener and method to call when the event is invoked. In this case, I want NewUser.registered to be executed when this event is triggered.\nAwesome!!!\nAnd we are done with setting up an event in AdonisJS. But we have not called this event or see it in action. How can we do that.\nFrom anywhere in your application call:\nconst Event = use(\u0026#34;Event\u0026#34;); Event.fire( \u0026#34;NewUser::registered\u0026#34;, user ); With the above code, I pulled Event from IoC, then call .fire on it with the name of the event that I want to execute as the first parameter and the parameters that the event handler(s) would receive are the arguments listed after the event name, in this case, only user is sent to NewUser.registered\nAnd that is it.\nEvents in an AdonisJS application.\nThis tiny feature is too useful, don\u0026rsquo;t render it useless.\nShalom! ☮️\n","permalink":"https://limistah.dev/posts/adonisjs-use-events/","summary":"Modern application development requires that some actions are carried out when a point of the application is reached.\nTasks like confirmation email, invoice generation, logging and profiling are few of things that requires to be carried out in specific regions of application flow.\nThese actions that are triggered are called Events. Events in modern application development make code execution after a web request to the server has been completed to be possible.","title":"AdonisJS - Event"},{"content":"AdonisJS was built for the NodeJS Artisans taking after the concepts of Laravel - The PHP framework for Artisans. AdonisJS did a great job porting these concepts into JavaScript, it maintains the namespace even though JavaScript does not support that, it using its own fast, easy and extendable view engine and many more, but some features of Laravel are not shipped with AdonisJS by default.\nThe IoC container in Laravel auto injects classes by inspection when a recognized namespace is Type Hinted. This makes route model binding easier with Laravel. In JavaScript, there is little that can be done to achieve this, so, there is a need for a custom approach to this.\nWhile I believe you have installed AdonisJS and have created an application with the adonis command, we will begin by creating a resource route:\nRoute.resource( \u0026#34;users\u0026#34;, \u0026#34;UserController\u0026#34; ).only(\u0026#34;update\u0026#34;); Next we can make a controller for the User from the terminal using:\nadonis make:controller UserController --resource The above command will create a file /app/Controllers/UserController.js with predefined methods for a cruddy endpoint. Awesome!\nAnd in the generated controller, lets return the id as JSON\n// /app/Controllers/UserController.js update ({request, response}) { return response.json({userId: request.id}); } Now, when we call PUT /users/1, it should return this:\n{ \u0026#34;userId\u0026#34;: 1 } Interesting\u0026hellip;\nTraditionally, we would have to lookup the id against the User Model, but, we would be doing this everytime for every route and that\u0026rsquo;s absurd.\nHow sweet could it be to have a middleware that does that by default, such that we can just call the middleware like so:\nRoute.resource(\u0026#34;users\u0026#34;, \u0026#34;UserController\u0026#34;) .only(\u0026#34;update\u0026#34;) .middleware(new Map([[[\u0026#34;update\u0026#34;, \u0026#34;boundRouteModel:App/Model/User,user\u0026#34;]]])); Awesome right?\nLet\u0026rsquo;s do this:\nFirstly, let\u0026rsquo;s make a new middleware:\nadonis make:middleware BoundRouteModel --resource And register it in the /start/kernel.js, and add the declaration to the nameMiddleware looks this:\n// /start/kernel.js // ...... const namedMiddleware: { //... \u0026#34;boundRouteModel\u0026#34;: \u0026#34;App/Middleware/BoundRouteModelMiddleware\u0026#34; } // ...... Awesome!\nNext in the /app/Middleware/BoundRouteModelMiddleware.js, replace the handle function with this piece of code.\nasync handle({ request }, next, [model, identifier, lookupField = \u0026#34;id\u0026#34;]) { if (typeof model === \u0026#34;string\u0026#34;) { // Use model directly if a string model = use(model); } else if (Array.isArray(model)) { // Maps through if an array model = model.map(async _model =\u0026gt; { // Model is a string, use directly if (typeof _model === \u0026#34;string\u0026#34;) { return use(_model); } else if ( _model.findByOrFail \u0026amp;\u0026amp; typeof _model.findByOrFail === \u0026#34;function\u0026#34; ) { // Model already contains an already imported model return _model; } // _model is not recognized by this code as usable or callable for its usage return null; }); } else if (model \u0026amp;\u0026amp; !model.$booted) { // model is defined from the middleware arguments but not actually a Model await next(); } if (model \u0026amp;\u0026amp; typeof model !== \u0026#34;string\u0026#34;) { const params = request.params; Object.keys(params).map(async (paramKey, index) =\u0026gt; { let modelForParam = model; // Get the model at the index if the model is an array if (Array.isArray(model)) { modelForParam = model[index]; } if (modelForParam \u0026amp;\u0026amp; modelForParam.$booted) { const paramValue = params[paramKey]; request[identifier] = await modelForParam.findByOrFail( lookupField, // The field to use for the lookup paramValue // Value passed from the request ); } }); } // call next to advance the request await next(); } The code above is doing pretty lot of things.\nFirst it assumes that the model passed to the middleware is a string.\nThen it attemps to import it from the IoC container, but if the model is an array, it loops through the array checking if the element is a string and tries to import it, or use it directly if it is an alreay imported Model.\nBut if the model is defined and is not recognized as a Model or an Array, the middleware is passed on to the next.\nFinally, the actual look up starts by checking if the model is not a string, picks up every item from the request parameter and assign a model at the index of the request parameter to it if model is an array, else if the model is a Model, it just looks it up and stores the result in the request using the specified identifier in the middleware initialization.\nYes, that is all to it.\nWhen next you want to use route model binding, you just use the middleware.\nConclusion It might feel like a heck of a task, but to me it is worth it. There is a library for this purpose and it uses the provider technique, you might want to check it out if you don\u0026rsquo;t like this approach.\nThanks for reading. 🙇🙇🙇\n","permalink":"https://limistah.dev/posts/adonisjs-route-model-binding/","summary":"AdonisJS was built for the NodeJS Artisans taking after the concepts of Laravel - The PHP framework for Artisans. AdonisJS did a great job porting these concepts into JavaScript, it maintains the namespace even though JavaScript does not support that, it using its own fast, easy and extendable view engine and many more, but some features of Laravel are not shipped with AdonisJS by default.\nThe IoC container in Laravel auto injects classes by inspection when a recognized namespace is Type Hinted.","title":"AdonisJS - Route Model Binding"},{"content":"First, install the Validator using the adonis command:\nadonis install @adonisjs/validtor Create a resource route\nadonis make:controller PostController --resource Define the route in start/route.js\n//.. Route.resource(\u0026#34;posts\u0026#34;, \u0026#34;PostController\u0026#34;) //.. Now, we can make a Validator for /posts/store\nadonis make:validator StorePost This will create a validator in /app/Validators.\nFinally, To define a validator for a specific route in the definition, do something like this:\n//.. Route.resource(\u0026#34;posts\u0026#34;, \u0026#34;PostController\u0026#34;).validator([[[\u0026#34;store\u0026#34;, \u0026#34;StoreUser\u0026#34;]]]) //.. PSSS: I want this as short as it can while still answering question regarding the final code. 🥂\n","permalink":"https://limistah.dev/posts/adonisjs-validators-with-resource-routes/","summary":"First, install the Validator using the adonis command:\nadonis install @adonisjs/validtor Create a resource route\nadonis make:controller PostController --resource Define the route in start/route.js\n//.. Route.resource(\u0026#34;posts\u0026#34;, \u0026#34;PostController\u0026#34;) //.. Now, we can make a Validator for /posts/store\nadonis make:validator StorePost This will create a validator in /app/Validators.\nFinally, To define a validator for a specific route in the definition, do something like this:\n//.. Route.resource(\u0026#34;posts\u0026#34;, \u0026#34;PostController\u0026#34;).validator([[[\u0026#34;store\u0026#34;, \u0026#34;StoreUser\u0026#34;]]]) //.. PSSS: I want this as short as it can while still answering question regarding the final code.","title":"AdonisJS - Using Validators with resource routes"},{"content":"Yes, this topic is worth a post and I will walk you through why the migration was inevitable, and how I had done it.\nFormerly, this website used to run on WordPress, the great PHP CMS. The design was made with the aid of Typography theme, I loved it!\nAll of the post prior to this date was written using the WordPress admin dashboard. I loved the Gutenberg Editor, it was so fluid and easy to work with. Overall my experience was great.\nThe Scenario Sometimes, installing software updates as end user could daunt the total experience of the user, in a case where the most beloved feature could be stripped or the presence of a bug somewhere which could be shipped unintentionally with the update. My own case was none of these, my experience was a total breakdown of the site, and I did not know why.\nFortunately, I trust so much that when installing WordPress on my hosting server, I opted for auto install new updates. This particular feature is a serial killer, and has not killed just this particular website twice, it also killed another website of mine too.\nHow it happened On a fateful day, I got a message from one of my readers stating that my website is down, I felt it was something not really serious, and as a techy person, I could fix it on time. I went into the WordPress admin dashboard and it was looking all well. I looked at everywhere including the logs, I found no clue on what I could do to make it right. It was an embarrassment to my personality (a tech guy).\nI got depressed and worried, this meant that I could not serve my website\u0026rsquo;s content anymore, even though they were somewhere in the cloud? I sought for a solution and I remembered, Gatsby is a very cool guy to deal with.\nAfter a lot of deliberations, and giving my mind the needed time and space, I finally concluded, I would gain a whole lot using Gatsby as a platform and framework than being on WordPress. Few of my reasons were:\nIt is blazingly fast! It was written with JavaScript (My first love!) It gives a full control and total customization Free hosting through Netlify The Path Learning Gatsby Installing Gatsby and choosing a theme Customizing the look. Device testing Pulling posts from the old WordPress setup Hosting the website Learning Gatsby Firstly, I had to learn Gatsby through its very resourceful documentation. This was easier than I had thought, and the features they provide enshrined my belief that it is the most modern static site generator, so far.\nIt has far new, but beautiful concepts for its routing, content fetching and asset management. All of these, I thought have been implemented with every level of user\u0026rsquo;s expertise in mind.\nWith Gatsby, you can go from writing zero codes by using themes which still give you the level of customization the framework provides, or by using a starter to emphasize your genius.\nI walked through the less technical path, I picked a theme that best suit my need.\nInstalling Gatsby and choosing a theme Upon ensuring that I have substantial knowledge of Gatsby, I moved to initialize a new project for my website and choosing a theme I know would be good for me. This process took me a week of love and hate process.\nJoke apart, I tried doing stuffs the genius way. Yes, many blogs and websites that run on Gatsby had done that, I thought, I would enjoy greater level of customization than using a theme, I was totally wrong.\nAfter weeks of trying to be Alan Turing, I dropped down my hat to that level of novice and succumb to use a theme. I was all around looking for a nice looking theme that I could use, then I stumbled upon Theme UI Blog Theme, which answers all of my questions.\nFortunately for me, Gatsby itself has a starter that is based this beautiful UI library, so I opted for it.\nSo starting with the installation of Gatsby itself using npm, and ensuring that it is installed as a global package:\nnpm install -g gatsby-cli This will install gatsby and make gatsby cli command available. Running gatsby --help should yield:\nThat\u0026rsquo;s one step forward.\nI later initialized a blog using the command below:\ngatsby new aleem.blog https://github.com/gatsbyjs/gatsby-starter-blog-theme The above command creates a new Gatsby website in aleem.blog directory, and also ensures that the starter that I would be using has integration of Theme UI\u0026rsquo;s integration. This simplifies my bootstrap process, still, I have the level of customization that I can ever think of with Gatsby using this starter.\nAfter a successful installation of the new website, the below command will run Gatsby in development mode:\ngatsby develop The result should be something like this:\nCustomizing the theme\u0026rsquo;s look and feel I love shades of purple, the theme comes with that by default, so I have no problem with the color at all. But, I do not like the way the page was structured, so I went to look deeper into the theme.\nIt happens that the theme expect that some files should be placed in a particular directory to override the default display.\nHere is a list of the files the theme uses to render its layout:\nFilename Location Purpose Bio Content src/gatsby-theme-blog/components/bio-content.js Author Biography Bio src/gatsby-theme-blog/components/bio.js Parent and data provider for Bio Content Footer src/gatsby-theme-blog/components/footer.js Posts page footer Header src/gatsby-theme-blog/components/footer.js Website Header Layout src/gatsby-theme-blog/components/layout.js Website General Layout Post Footer src/gatsby-theme-blog/components/post-footer.js Footer for the post page Post src/gatsby-theme-blog/components/post.js Single Post display renderer Posts src/gatsby-theme-blog/components/posts.js All Posts listing SEO src/gatsby-theme-blog/components/seo.js SEO utility Switch src/gatsby-theme-blog/components/switch.js Dark Mode Switch Color src/gatsby-theme-blog/gatsby-plugin-theme-ui/color.js Color configuration Typography src/gatsby-theme-blog/gatsby-plugin-theme-ui/typography.js Theme UI Typography As seen above, all these are what renders our new site. I took enough time to work on each file to give me what I wanted.\nThe posts are expected to live at content/posts, I maintained that. The header is from Theme UI Recipes, the homepage is just a vertical grid, nothing complicated.\nDevice testing Considering the various users that would stumble on the site, I ensure that while adding my customizations, I was checking for responsiveness all round. But that was not enough. I took time to get across screen sizes to see how it really look and feel.\nNot until I found the fluidness, I tweaked until I got something like this:\nPulling posts from the old WordPress setup Fulfilling all righteousness, I did not write any code to pull the posts from WordPress. I did the posts migration manually, through copying and pasting.\nKnowing when to write a program to simplify a task, and actually doing the task manually is a wisdom of its own. I had a few posts(less than 10), and I thought it would be best I do that manually which did save me some time from testing and debugging if I had written a program/library for it.\nNonetheless, if I would write a library to simplify the task, this is how I would do it:\nExport all posts from WordPress using their export tool. Get a suitable XML to HTML parser library on NPM. Get a suitable HTML to MD library on NPM. Parse the exported XML to HTML then to MD before saving it to the disk with each file named with the title of a post and containing the content of the post. The above does not take care of assets like images and media. I faced an issue with asset referencing while pulling the posts, as I had done the posts manually, I downloaded all of the assets and placed them in the assets folder of the Gatsby setup.\nIt was not easy doing it manually, I believe it would be interesting to have a library that would do all of these to make future migrations easier. I might someday give it a thought.\nHosting the website With Netlify, I faced no issue hosting the website and it was for free.\nCreating a Netlify account is a breeze considering the amount of options that they provide, my account with them uses the Github integration, and it was pretty fast.\nAfter my authentication and confirmation, I created a new application selecting Github as my code source. Netlify\u0026rsquo;s bot the rest of the process (building and serving the application).\nIn a matter of few minutes, the website came to live under a subdomain of Netlify which is the name of the application. The name looked really scary, so I changed that to reflect my name (aleemisiaka, of course).\nAnd there we have it.\nAt the time of this writing, I have not pointed my domain to the Netlify application. I hope to move my domain registrar to Netlify, at least a means of saying \u0026ldquo;Thank you!\u0026rdquo;\nConclusion It was not a rocket science. It was a matter of what needed to be done, and had to be done.\nI am so glad to be part of the Gatsby ecosystem, I would look into projects that I can contribute to. If you have one, 📭 shoot me a dm!\n🙇 Salut!\n","permalink":"https://limistah.dev/posts/migrating-from-wordpress-to-gatsby/","summary":"Yes, this topic is worth a post and I will walk you through why the migration was inevitable, and how I had done it.\nFormerly, this website used to run on WordPress, the great PHP CMS. The design was made with the aid of Typography theme, I loved it!\nAll of the post prior to this date was written using the WordPress admin dashboard. I loved the Gutenberg Editor, it was so fluid and easy to work with.","title":"Migrating from WordPress to Gatsby - The Journey"},{"content":"Let\u0026rsquo;s be guided, we are not getting naughty here.\nNevertheless, a programmer is a human, he has his feelings and emotions which is interesting. The direction he puts these uniquely human traits makes him more interesting - towards the code and software, of course.\nI\u0026rsquo;ve been through a variety of programming languages, which always turned to be that the deeper I dive into a language, the more \u0026ldquo;aha\u0026rdquo; moment I get. I always feel like I have gotten what I wanted and have to look no further to attract the priceless attention of my fellows.\nI saw unique features, improvements in another, etc. scattered all around these languages. Each time that I had noticed a better way of doing things other than that of a previous language, I would give myself the accomplishment smile thinking that I found what I wanted my quest for a sexy language was over, unlucky for me, it has been a repeating pattern.\nTake a glance look at the below elixir snippet, where returned values from function calls can be pipped instead of being wrapped in parentheses or pass down as parameters.\nThe first time that I saw that piece of feature, I felt a big grin inside of my small intestine, a sweet sensation of coolness, that is what all programmers love to have aside from the actual \u0026ldquo;sexiness\u0026rdquo; inborn with them.\nWhat I believe many other programmers like me failed to realize is a fact that these languages are tools, same with libraries and frameworks. Looking through the eye of a Mechanical Engineer, I understand that a Pipe Wrench even though it looks like, but can not work as an Adjustable Spanner, though they can be adapted in place of each other in some cases.\nPHP/Laravel, NodeJS/Express, Java/Spring, Python/Django, etc. should not be and can never be a debate of which is more beautiful and sexy, it is just baseless. Same way as asking Beyonce and Rihanna who\u0026rsquo;s more beautiful? Really?\nI understand the point that a programmer\u0026rsquo;s delight is to look more sophisticated than his peers in terms of tooling and machine. But a question is, does this really worth the fight? Do we have to keep asking who has more fan base, use case, GitHub stars, recently used and likes and not which actually gets these kinds of tasks done and which pays more?\nAll the tools that exist for us as a programmer have tradeoffs. Inventors of these tools tried to solve one major problem with them, and we could notice that these tools have thrived in their domain and would continue to thrive as long as there is no better replacement for it.\nC might be outdated, Fortran might be dead, Cobol could be seen resting in Valhalla, does that mean that people have forgotten about them? Does that mean the works done with them too are dead? Do we have to rewrite all the whole of an application just because of a new trendy tool?\nNo, we do not and we should not. To those that find those old fashioned languages interesting, let them have their time and space to prove their beauty. Even transitioning from an old language to a new one requires the knowledge of both languages, and does not translate to that old language being dead.\nUntil we start thinking of programming as a business and a means to an end, which actually takes out the sexy thoughts of a macho man, the quest of being that sexy programmer will always remain.\nI think we all should start seeking for what actually brings in the bags and not the bugs. We can wander around these languages and expand our horizons. They should not be as a result of seeking to be more fashionable that would translate to the programmers looking sexier. It should be about the bank account, and not more macho.\n","permalink":"https://limistah.dev/posts/programmer-sexiness-quest/","summary":"Let\u0026rsquo;s be guided, we are not getting naughty here.\nNevertheless, a programmer is a human, he has his feelings and emotions which is interesting. The direction he puts these uniquely human traits makes him more interesting - towards the code and software, of course.\nI\u0026rsquo;ve been through a variety of programming languages, which always turned to be that the deeper I dive into a language, the more \u0026ldquo;aha\u0026rdquo; moment I get.","title":"Programmer's Sexiness Quest"},{"content":"In this post, we will explore how to set up a NodeJS app on Amazon’s Lightsail instance. We will also explore setting up a CircleCI job for a NodeJS project, use Nginx as a web server, setup SSL for the server, and allow a local machine to access the remote server.\nPrerequisites AWS Lightsail instance AWS Route53 domain (not mandatory) CircleCI account Github repo for the NodeJS project. Let’s begin!\nSpinning up an instance I have spun up a $5 instance with my account, it comes with 1 GB RAM, 1 vCPU, 40 GB SSD. Below is the image configuration I used while setting up the instance:\nAWS Lightsail is configured to use a dynamic IP each time the server is restarted, a basic sudo reboot would give the server another IP address, we should configure a static IP for our server. This will save us the dynamic IP rotation.\nAdding static IP to an instance On the instance dashboard that you have created, click on the Network tab, then Attach static IP button.\nIt will toggle a select input to pick from existing static IPs that are not yet attached to an instance or create one. I have created one already so I only selected from the list, then the green button to confirm my selection. If you do not have one, enter a name for the IP then click on create button to create one, after which you can then attach to the instance.\nEither way, we now have a static IP attached to our Lightsail server. Cool isn’t it?\nAllowing SSH access from a local machine AWS Lightsail comes with a web-based terminal that runs like a regular UNIX terminal. I do not feel at home with it, so, I will be setting up an SSH access for the instance. This will improve my productivity working with the VPS.\nFrom the Lightsail home page, choose the instance you are working with, then click on the connect using SSH button.\nA new browser window should pop up, you should see:\nNext, from the command prompt run:\nsudo nano ~/.ssh/authorized_keys You should be greeted with a page containing default ssh key for your instance(more on this later). We will be adding our local machine’s public ssh key below the key already in the authorized_keys file.\nFrom your local machine, create an ssh key or use an existing one. I am using an already existing one, the command below will copy my id_rsa.pub file to my clipboard.\nxclip -sel clip \u0026lt; ~/.ssh/id_rsa.pub Back in the web terminal, from the interface, there is a clipboard icon, click on it a text area should appear at the top of the icon. Click into the text box, then press Ctrl+V or Cmd+V to paste the contents from your local clipboard into the browser-based SSH client clipboard. Right-click any area on the SSH terminal screen to paste the text from the browser-based SSH client clipboard to the terminal screen. Easy, right?\nTo exit the edit window, press Ctrl+X, press Y to confirm the modification, and ENTER to complete the process.\nFinally, reboot the server to reflect our changes:\nsudo reboot From our local machine’s terminal, we can run:\nssh ubuntu@LIGHT_SAIL_STATIC_IP Replace LIGHT_SATIL_STATIC_IP with the static IP attached to your instance. If all goes well, you should be welcomed with the same message as the web terminal does. Well done!!!\nFinally, for our AWS setup, we can go further to mask our static IP to a domain name. AWS Route53 can do that for us easily, I won’t be covering that here, you can look up this article for the process.\nInstalling NodeJS To run our NodeJS code on the server, we need to install NodeJS runtime. I followed a tutorial from Tecadmin, here are the commands to do the installation:\nsudo apt update sudo apt install -y mongodb sudo systemctl enable mongodb sudo systemctl start mongodb sudo systemctl status mongodb The last command should give a working systemd status with a green highlight:\nAdding a new user account for deployment To deploy your app, you will require a copy of your NodeJS app code on the server. It is a good practice to have a user account whose sole purpose is to serve your app.\nTo create a new user for your Ubuntu server, running the below command will create a new user dkapi it will also create a home directory for the user at /home/dkapi\nsudo useradd -m -d /home/dkapi dkapi Next, you should add a password for the user that you just created, the command below will prompt for a password to be used for dkapi.\nsudo passwd dkapi You should be able to run commands as sudo user with the newly created user account. The below command will allow that to be possible:\nusermod -aG sudo dkapi Now, you can switch to the user you just created with:\nsu - dkapi Provide the password you entered when creating the account, you should find a prompt with a $_ sign, that’s a success. Cool!\nCreating ssh keys for Github access I followed the guide by Github to create an SSH key from here. Then, add the generated ssh to your Github account using the guide from here. Finally, we can test our Github SSH set up with the guide from here.\nPulling the NodeJS codebase from Github We are here, you can now pull your NodeJS code from Github. This is basically cloning the repo, and this can be done easily with:\ngit clone GITHUB_SSH_URL ~/app The repo will be cloned into ~/app directory\nFollow the guide here to install Yarn on the instance, after that you can install the dependencies using yarn command.\nRunning NodeJS app in the background For perfect integration with Nginx, you need to run your NodeJS app in the background. Packages like forever, pm2 gives this process a breeze. But, for me, I think using a systemd service is more appropriate, as I can start, stop and enable autostart of the service whenever the server is restarted. With the SSH access,systemd service has an edge over an npm package.\nA systemd service are instruction files created at: /lib/systemd/system/ and the files are to end with .service extension. Full details about systemd can be found here. Let’s create one for ourselves.\nFirstly, run sudo nano /lib/systemd/system/dkapi.service you can name dkapi.serviceanything, just ensure that it ends with a.service` as the file’s extension.\nThe content of the file should look like:\n[Unit] Description=APP API Service After=network.target StartLimitIntervalSec=0 [Service] Type=simple Restart=always RestartSec=1 User=dkapi ExecStart=/usr/bin/node /home/dkapi/app/start.sh WorkingDirectory=/home/dkapi/app StandardOutput=syslog StandardError=syslog SyslogIdentifier=dk-api Environment= NODE_ENV=production [Install] WantedBy=multi-user.target Attention should be paid to User=dkapi that should be the user account you created for the deployment, ExecStart=/home/dkapi/app/start.sh this is the start command for the service, basically calling an executable start.sh file. The file in my own case contains:\nyarn start Ensure that this file can be executed, use sudo chmod +x start.sh for this.\nThe WorkingDirectory=/home/dkapi/app should be the directory you pulled the files from Github into. And Environment= NODE_ENV=production to tell the environment variable you want the service to have.\nNext, enter Ctrl+X then ENTER to save the file. Now, you have a service systemd that can be used to manage your NodeJS server instance.\nTo enable the server to start when the server boots up we can do: To do systemctl enable dkapi Now, we can manually start our service with:\nsudo systemctl start dkapi\nGreatness!!!\nInstalling Nginx webserver NodeJS app only creates a local server, to accept request over the internet, we need a web server that will make that a breeze. We can do Apache, but I love Nginx for its simplicity, we will be installing and configuring one for this setup.\nFrom your SSH accessed server terminal run the below command to update the installed packages:\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade After that, you can install Nginx with:\nsudo apt install nginx A successful installation will give you:\nVisiting the static IP attached to this instance gives the default welcome page for a successful Nginx installation. Awesomeness!!!\nNginx configuration for NodeJs server routing To ensure that requests are passed to your NodeJS app, you need to have the default Nginx configuration block point to your the local URL of the app.\nWe will begin this by configuring our Nginx webserver.\nChanging directory to nginx installation directory with: cd /etc/nginx/ In the sites-available directory, copy the default config file to default.bak using sudo cp default default.bak, then use sudo nano ./sites-available/default to edit the content of the file.\nHere is a configuration that receives a normal request from the client then transfer it to the node server we have set up above.\nserver { server_name SERVER_NAME_OR_STATIC_IP; location = /favicon.ico { log_not_found off; access_log off; } location = /robots.txt { allow all; log_not_found off; access_log off; } location / { proxy_pass http://localhost:3000; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#39;upgrade\u0026#39;; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_cache_bypass $http_upgrade; } access_log /var/log/nginx/dkapi-access.log; error_log /var/log/nginx/dkapi-error.log error; } Change the SERVER_NAME_OR_STATIC_IP to your server name that points to the static IP, or just use the Lightsail server static IP.\nIf all went well, visiting the server name or static IP should yield the default response for our app. Mine:\nAdding free SSL by letsencrypt Sometimes, accessing our NodeJS app using normal unsecured HTTP would throw a warning screen or will not resolve. This is a security feature implemented by web browsers and hosting providers. To this, I always install SSL for nginx servers. We will be doing a setup with let’s encrypt as generally done.\nRun the below command to install certbot package.\napt-get install software-properties-common add-apt-repository ppa:certbot/certbot apt-get update apt-get install python-certbot-nginx To set up an SSL for your domain, you can do:\ncertbot --nginx It will check the CN (common name) in the existing Nginx configuration file, if not found then it will prompt you to enter the domain name.\nCertbot automation is smart! It will take care of all the necessary configurations to make your Nginx ready to serve over https.\nNot so fast, but we now have SSL support for our nginx web server at the specified domain name.\nTo allow HTTPS requests to come through into the VPS instance, you have to modify the firewall from the Network tab on the instance dashboard. Click on Add another, select HTTPS, then click on the green tick.\nIt should look something like this:\nAnd that’s it! Full HTTPS support without an extra.\nTaking things further If you have a build process and a Continous Integration, you should be thinking of setting it up. This would protect against the round trip of pulling and restarting our app on the server.\nI assume that the project has been set up on CircleCI already using a Github account. Let’s move forward.\nf you notice from the bottom page of the Lightsail server instance dashboard Connect tab, it states, “You configured this instance to use id_rsa (key_region) key pair.” This is a secret SSH key and can be managed from https://lightsail.aws.amazon.com/ls/webapp/account/keys.\nThe keys from the page above are private keys whose public keys are included in instance setups. Lucky enough, it has been included in my own setup and it is what CircleCI is requesting to access my server remotely. It is not included in the authorised_keys of the user account that you have created, you are to add the public key in there.\nTo get the content of the file, firstly download the ssh key attached to the instance, next use cat command to show the content of the file. In my own case:\ncat LightsailDefaultKey-us-east-1.pem On the project page in CircleCI dashboard, click the settings icon close to the project, then SSH Permissions. Click on the blue Add SSH Key button enter the domain name for the key, then paste the copy private key into the Private key input, click the light blue Add SSH Key button to complete the process.\nNew user accounts do not come with the private ssh configured, we have to manually add this ourselves. To do this, we can generate a public key from the downloaded private key using:\nssh-keygen -y -f ~/.ssh/lightsail.pem \u0026gt; ~/.ssh/lightsail.pem.pub Then you can copy the content of ~/.ssh/lightsail.pem.pub to the authorized_keys file of the account for your app. Simply:\nsu — dkapi sudo nano ~/.ssh/authorized_keys Paste the copied public key into the file, then Ctrl + X to save.\nBelow is the CircleCI config for my own project, yours might be different. The important part here is the command section.\nversion: 2 jobs: staging: docker: - image: circleci/node:10 steps: - run: name: Deploy API command: ssh -o \u0026#34;StrictHostKeyChecking no\u0026#34; api@staging.datingkinky.com \u0026#34;cd ~/app; git pull; yarn install --production; sudo systemctl restart dkapi\u0026#34; workflows: version: 2 staging: jobs: - staging: filters: branches: only: develop The emphasis is on this:\nssh -o \u0026#34;StrictHostKeyChecking no\u0026#34; dkapi@staging.datingkinky.com \u0026#34;cd ~/app; git pull; yarn install --production; sudo systemctl restart dkapi\u0026#34; To note from the above command:\ndkapi@staging.datingkinky.com: We want to connect to the instance but as the dkapi user we created. cd ~/app: Change directory to the app directory we pulled the code into. git pull: Pull the code from Github to keep the code updated yarn install — production: Install dependencies that might have just been included. sudo systemctl restart dkapi: Restarts the dkapi service running the NodeJS app in the background using systemd service A perfect process right?\nPush a new code, let CircleCI do your deployment and restarting the node app for you. Sooo cool!!!\nBeautiful? Not yet!\nGotcha I ran into a failing build, which is a result of the systemctl command. The error states\nno tty present and no askpass program specified Basically, we have to bypass systemctl asking for password whenever we run it from a trusted machine. I found the solution from Stackoverflow.\nNow, we have to ssh as a root user, then run:\nsudo visudo The command will pop a file for editing, it is a delicate file that can mar all the effort we have put into the setup. What we have to do in here is to go to the end of the file to add:\ndkapi ALL = NOPASSWD: /bin/systemctl Replace dkapi with the user you created in for deployment.\nUse Ctrl+X to exit the file. Then, we can rerun the failed CircleCI workflow or push a new code to test the setup.\nIt ran successfully in my own case, I believe yours too would have been successful.\nAwesome!!! Highlights Spinup a Lightsail instance Add a static IP to the instance Allow SSH access from a local machine Installing NodeJS Installing MongoDB Adding a new user account for deployment Creating ssh key for Github access Pulling the NodeJS repo from Github Running NodeJS app in the background Installing Nginx webserver Nginx configuration for NodeJS server routing Adding free SSL by letsencrypt Integrate CircleCI CircleCI integration gotcha Conclusion Following through this post, we have configured a non-existent AWS Lightsail server instance to run a NodeJS app. We have been able to set up the code to run a systemd service for the app to be able to run in the background.\nWe added SSL and then finalized the setup with a CircleCI integration and a fix to a known error that could cause the build process to fail.\nI will like to learn if there is any part that could be improved, know about a bug that stops your own setup and learn more about the AWS Lightsail Servers.\nThank you for reading. I appreciate your patience!!!\n","permalink":"https://limistah.dev/posts/nodejs-lightsail-setup/","summary":"In this post, we will explore how to set up a NodeJS app on Amazon’s Lightsail instance. We will also explore setting up a CircleCI job for a NodeJS project, use Nginx as a web server, setup SSL for the server, and allow a local machine to access the remote server.\nPrerequisites AWS Lightsail instance AWS Route53 domain (not mandatory) CircleCI account Github repo for the NodeJS project. Let’s begin!","title":"Complete NodeJS App Setup  on an AWS Lightsail VPS"},{"content":"The very first task that we do while starting to program is naming from the creation of directory to files, to classes, to functions. Name is everywhere, we cannot escape it, and we do so much of naming, we should be fulfilled if we can do it better.\nBeginners do marvel at how the seniors snap out great names that fit the context to be applied. How the names are crafted seem like magic, they end up seeing themselves not so close when they see that wack name their brain could best provide.\nFortunately, Micheal Jordan was not born a perfect athlete, and he learned it all. We can learn how to craft proper names by following a set of guidelines laid out by experts in the field of computer programming.\nIn this series, Clean Code – Rules For Name Crafting, we will be learning how to write proper names in our codebases and the rules to guide our thoughts when we require a name. All lessons are taken from Clean Code by Robert C. Martin, and the lessons here are going to be illustrated using JavaScript from the Java Robert used in his book.\nRobert stated below rules, and we also see them as essential for naming considerations in our codebase:\nAlways use intention revealing names Avoid disinformation Make meaningful distinctions Use pronounceable names Use searchable names Avoid encodings Avoid mental mappings Do not be cute Pick one word per concept Do not Pun Use solution domain names Use problem domain names Add meaningful context Do not add context gratuitously It is an undeniable list with thoughtful and straightforward rules, and we will be going over all of them throughout the next few posts.\nBy the end of this series, we would be informed of why we make bad names, and how we can make good ones. Let us get started with the Rules for name crafting – Part One.\nWe will meet on the other side!\n","permalink":"https://limistah.dev/posts/clean-code-series/","summary":"The very first task that we do while starting to program is naming from the creation of directory to files, to classes, to functions. Name is everywhere, we cannot escape it, and we do so much of naming, we should be fulfilled if we can do it better.\nBeginners do marvel at how the seniors snap out great names that fit the context to be applied. How the names are crafted seem like magic, they end up seeing themselves not so close when they see that wack name their brain could best provide.","title":"Clean Code – Rules For Name Crafting (Series)"},{"content":"Currently, I find my self reading Clean Code by Robert Cecil Martin, and just completed the first three chapters. I am taking my time to digest the genius work of Robert, and I see it as a need to share what could be understood in just the first 50 pages of the book.\nTo Robert, spending time to write the right code is an investment in the future maintenance time of that code. You can probably relate how a poorly written code has made you brainstorm for hours if not days before you could make that simple one-line change.\nWhile some people see it as moving really fast and not paying the necessary attention to their codes, he mentioned how writing good code is like writing a novel. The flows, structure, and content arrangement should be perfectly laid out to make the reader not need a second glance before comprehending the information the writer is passing.\nIn his book, Robert stated a conundrum: “All developers with more than a few years of experience know that previous messes slow them down. And yet, all developers feel the pressure to make messes in order to meet deadlines.” Which he further agree to that true professionals the second part of the conundrum is actually wrong, as a saying goes no amount of wrongs can make a right.\nIf you have been in the freelance space, you probably must have experienced the second part of the above conundrum. We tie productivity to shipping in the least possible time, and not considering that the time we did not pay to think through writing good codes would be demanded the next time we visit them again.\nYou know you are working on clean code when each routine you read turns out to be pretty much what you expected. You can call it beautiful code when the code also makes it look like the language was made for the problem: Ward Cunningham\nCodes should be pretty straight forward and do what they state, you nod at good code when you read them from the first glance. Part of our responsibility is to make the language look simple. With Ward\u0026rsquo;s statement, it is not the language that makes programs look simple. It is actually the programmer that make the language appear simple!\nTo Robert, the ratio of time spent to write new code to the time spent reading old codes is 10:1. Experienced programmers can tell how much they have gone back and forth writing, deleting and commenting out codes in an effort to write a new one.\nIt takes constant practice to write good codes, it does not have to happen from the very first time of writing the codes. It could come after the code has been completed in an optimization process, the goal is not to leave a messy campground.\nChapter Three of the book commence the lesson on writing good codes, beginning with tips for writing a good function. There is a wealth of knowledge buried in this chapter, I will be sharing my thoughts and understanding about this some other time.\nWe are authors, and one thing about authors is that they have readers. Indeed, authors are responsible for communicating well with their readers, the next time you write a line of code, remember you are an author, writing for readers who will judge your effort. - Robert Cecil Martin\nBe an artist, draw beautifully. Be a writer, write understandably. Be a poet, write eloquently. Be a programmer, write good codes.\n","permalink":"https://limistah.dev/posts/clean-code-review/","summary":"Currently, I find my self reading Clean Code by Robert Cecil Martin, and just completed the first three chapters. I am taking my time to digest the genius work of Robert, and I see it as a need to share what could be understood in just the first 50 pages of the book.\nTo Robert, spending time to write the right code is an investment in the future maintenance time of that code.","title":"Clean Code – Brief Review"},{"content":"MongoDB ships with an easy mongod CLI command to start its server. For Linux users, there is an added level of flexibility using the Systemd service to manage foreground and background processes. To start a MongoDB server on the boot of a Linux machine, it is as easy as registering a service with systemd using:\n$ systemctl enable mongod.service Switching from a Linux machine to Mac, and after the successful installation of MongoDB, surely, there is a need to start the command in the background while the development process continues.\nTo this, there are --fork, --quiet, and --syslog. Which are command line parameters for the mongod command.\nTo further ease the use of this our new discovery, we can create an alias in our .bashrc file by appending this command at the end of the file:\n$ echo \u0026#39;start-mongo = sudo mongod --fork --syslog --quiet\u0026#39; \u0026gt;\u0026gt; ~/.bashrc Now, at the start of our Mac machine, we can do start-mongo to launch our mongod service.\nEasy enough! 🍧\n","permalink":"https://limistah.dev/posts/mac-mongodb-in-background/","summary":"MongoDB ships with an easy mongod CLI command to start its server. For Linux users, there is an added level of flexibility using the Systemd service to manage foreground and background processes. To start a MongoDB server on the boot of a Linux machine, it is as easy as registering a service with systemd using:\n$ systemctl enable mongod.service Switching from a Linux machine to Mac, and after the successful installation of MongoDB, surely, there is a need to start the command in the background while the development process continues.","title":"Running mongod service in the background – MAC OS"},{"content":"Beginners do believe there is a perfect structure for setting up a project, experienced programmers know this is so far from the truth. A perfect structure is only perfect for a specific project, while project requirement varies across specifications, a perfect project setup varies across projects.\nLooking further down project specifications, there are repeating patterns and procedures that make all the projects look like they are all doing the same thing. Create Read Update Delete (CRUD) was invented for this purpose, many applications are doing at least one of CRUD action, a tour around would reveal the truth about this.\nSoftware development frameworks came as a result of abstracting the repeating patterns in applications. For PHP there is Laravel, Yii, CakePHP, Codeigniter, etc. NodeJS has a handful of its own from Sails, to Hapi to FeathersJS, and likes, even CSS is not excluded with Bootstrap and Foundation as examples of frameworks based on it.\nSome application requires subtleness and fluidity that frameworks do not provide or provide too much of. An experience with writing a single about me web page does not demand the pulling of almighty Laravel. Same way, we can\u0026rsquo;t generically think we could build a really large app with the setup provided by a framework.\nWhile Development Framework is good, taking away the responsibility of having a section of our code being tested across use cases. We should not forget that the Framework could bloat or run out of use case for some project specifications. Sometimes, Frameworks provide too many details than we need or lesser than it we require.\nAs the goal of third-party applications is to reach the use case of a wider range of users. There is a good number of people that fall out and some do fall too deep into the use cases they have to provide. A good software delivery skill is being consciously aware of when to use frameworks and when to bootstrap a custom codebase specifically designed for the situation at hand.\nSome frameworks fall too good that we tend to believe they can fit into all the use cases we can think of. This thinking obscures the fact that there is no size that fits all. If the specs are critically analyzed, we will find out that the application structure falls in one of:\nThis is too little to go into a framework This fits perfectly into this/that Development Framework No, there is no known framework setup that can support this. Before reaching a conclusion using the above points, there some preliminary questions that should be asked. Questions like:\nHow much control does the core code require? Will there be frequent updates than the framework release timeline intervals? Can errors/bugs be looked over, is workaround allowed to be used Does the performance of the framework satisfy the application requirements? How knowledgable is the team/programmer handling the development? Could reported errors be delayed till the framework actually provide a fix? Does the software license provided by the framework not conflicts with the requirement of the project? The questions above are just a generic overview, some projects will have their own specific requirements which will pose some more interesting questions to reach a conclusion about the use of third party code in the application development.\nFrameworks are so good generally, but they are not always the perfect solution for some application development process. Take extreme caution when making a great decision for the base of an application. Rewrites are never interesting, they look like walking past a previous stage in life, and humans do not like repeating a life\u0026rsquo;s stage.\nBe guided!\n","permalink":"https://limistah.dev/posts/frameworks-not-perfect-solution/","summary":"Beginners do believe there is a perfect structure for setting up a project, experienced programmers know this is so far from the truth. A perfect structure is only perfect for a specific project, while project requirement varies across specifications, a perfect project setup varies across projects.\nLooking further down project specifications, there are repeating patterns and procedures that make all the projects look like they are all doing the same thing.","title":"Frameworks - Not always the solution"},{"content":"This is the first of the series – Clean Code – Rule For Name Crafting Series. In this post, we will be dealing with the first three rules stated by Robert in his Clean Code book and they are: always use intention revealing names, avoid disinformation, and make meaningful distinctions.\n#1: Always Use Intention Revealing Names To some profession, giving names that do not reveal what the item/object is about might be a norm, we have seen astroids named Iris, and a human named Mars. Imagine that you are writing an Airport Flight Management System, what could client or customer mean generally in the app? These are names that could apply differently based on the context that they are being used.\nGeneralization should be avoided when naming variables, classes, methods/functions. Names should mean what they actually are wherever they appear. You could name the client/customer variable to be passenger while in a Flight class and customer while in the Payment class. These are valid names that reveal what they are intended for.\nAnother version of non-intention revealing names is the use of first letters of the name. Take const p = this.customer, this pollutes that section of the codebase with mappings of a single alphabet, remember when you have to look up to where a variable is declared, you have a bad variable name.\nLet us consider this little code:\nconst getThem = () =\u0026gt; { const list1 = new Array(); for (let x in theList) { if (theList.hasOwnProperty(x)) { if (x[0] === 4) { list1.push(x); } } } return list1; } It is obvious that there is no indication of what this code is doing. We have to ask questions like:\nWhat is the Them in getThem? What is the significance of the first element in each item of the list How can the returned value for this be consumed? A perfectly named and intention revealing code should answer all of these for you with you not asking.\nCheck out this modified version of the code:\nconst getActiveUserGroup = (userGroups) =\u0026gt; { const activeUserGroups = new Array(); for (let userGroupId in userGroups) { if(userGroups.hasOwnProperty(userGroupId)) { const userGroup = userGroup[userGroupId]; if (userGroup[STATUS_INDEX] === ACTIVE_SATATUS) { activeUserGroups.push(userGroup); } } } return activeUserGroups; } The modified version of the code is so explanatory, in that it tells what it does and reads like a story. Debugging this code does not require a preprocess function of the brain.\n#2: Avoid Disinformation Using the code in the above section and looking through theList and list1, what do they tell, actually? What information are they telling our readers by list1?\nAs programmers, we have to avoid giving information that actually does tell what we don\u0026rsquo;t mean. You should avoid using an Array where you actually meant an Object, and do not use an Object where it is a String. Let the name tell what they actually do.\nSpelling similar concepts similarly is information. Using inconsistent spellings is dis- information. You should not use names whose differences are not easily noticed, do not use names like XYZControllerForEfficientHandlingOfStrings and XYZControllerForEfficientStorageOfStrings, how hard it is to spot the difference, very hard you see? One example of uninformative names is using f for fund, as we do while iterating a list of funds. - Robert C. Martin\nWe can consider the below snippet:\nlet u = user; let n = \u0026#34;name\u0026#34; let cn = \u0026#34;\u0026#34; if (n === u.name) { cn = n } else { n = u.name; } The above code can is best understood at the very point it was written. It is likely impossible for the reader to actually know what each variable means, without looking further beyond this code block. A better version would look like:\nlet searchName = \u0026#34;name\u0026#34;; let foundName = \u0026#34;\u0026#34; if (searchName === user.name) { foundName = searchName; } else { foundName = user.name; } Good variable naming is a critical skill that could make a codebase look like a dump yard.\n#3: Make Meaningful Distinctions You probably have been in a situation where you do not want a variable\u0026rsquo;s value to be overwritten, still, you need the same name while in the same scope. How did you handle that?\nSome programmers will call the first occurrence: theUser and the second user, some would do user1 and user2. What I used to do is user and deeper in the code I would do _user, all these are a bad variable naming skill.\nUsing noise words, numbers and special characters does not help to convey information about a name. Whenever there is a need for a name, you should know that there has been an activity that is a result of a reaction triggered by that section of the code. No two names can look alike if you have followed the first rule intention revealing.\nNoise words are meaningless and create redundancy. A variable should not appear in a variable name, a class should not appear with a class name, usernameString is not better than using just username, good code structure should not allow types to juggle at will.\nIn the absence of specific conventions, the variable moneyAmount is indistinguishable from money, customerInfo is indistinguishable from customer, accountData is indistinguish-able from account, and theMessage is indistinguishable from message. Distinguish names in such a way that the reader knows what the differences offer. - Robert C. Martin\nConclusion We have just learned the first three rules that guide good variable naming. We have come to realize _user is a bad variable name as well as theUser. Initial alphabet of a name is not revealing the intention. Every time you are thinking of a name, you should consider the information you want your future self to know about that name.\nIn the next post, we will continue picking up more rules as defined by Robert. Write good names, and reveal your intentions, write beautiful codes!\n","permalink":"https://limistah.dev/posts/clean-code-name-crafting-one/","summary":"This is the first of the series – Clean Code – Rule For Name Crafting Series. In this post, we will be dealing with the first three rules stated by Robert in his Clean Code book and they are: always use intention revealing names, avoid disinformation, and make meaningful distinctions.\n#1: Always Use Intention Revealing Names To some profession, giving names that do not reveal what the item/object is about might be a norm, we have seen astroids named Iris, and a human named Mars.","title":"Rules For Name Crafting – Part One"},{"content":"Passing down functions as event handlers down to children components is a norm in the react world. It eases the communication flow, as the saying goes props down, functions up.\nThings get a little bit tricky when using a React Classical component. Functions have to maintain their scope for proper interaction with their declared class properties. You might not be lucky sometimes, so there has been a couple of workaround for this.\nBind that function! Yes, this is what happens. You bind function passing the scope of the class they have been declared in.\nI am considering a simple ButtonClickCounter component:\nimport React, { Component } from \u0026#34;react\u0026#34;; class ButtonClickCounter extends Component { constructor(props) { super(props); } state = { clickedCount: 0 }; handleButtonClick() { const clickedCount = this.state.clickedCount; console.log(\u0026#34;This Button Has Been Clicked: %s times\u0026#34;, clickedCount); this.setState({ clickedCount: clickedCount++ }); } render() { return ( \u0026lt;div\u0026gt; \u0026lt;span\u0026gt;Button clicked \u0026lt;em\u0026gt;{this.state.clickedCount}\u0026lt;/em\u0026gt; times\u0026lt;/span\u0026gt; \u0026lt;button onClick={this.handleButtonClick}\u0026gt;Button\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; ); } } If you try to run this code, things will work fine till you try hitting the Button. You will get a Reference Error, what did we do wrong?\nIt happens that the function will be triggered in another scope where I do not know - that is for React to determine. But, I need to communicate with our local state, so, I should bind that function!\nBinding on the component Easily I could bind the ButtonClickCounter\u0026rsquo;s scope to the handleButtonClick handler by calling .bind while passing it.\nThe above code would provide a lasting solution to my scope problem, easy!\nBUT!!! What if I have some other button that uses the same function as an event handler? I would have to do .bind(this) on all the references?\nconst render = () =\u0026gt; { return ( \u0026lt;div\u0026gt; \u0026lt;span\u0026gt; Button clicked \u0026lt;em\u0026gt;{this.state.clickedCount}\u0026lt;/em\u0026gt; times \u0026lt;/span\u0026gt; \u0026lt;button onClick={this.handleButtonClick.bind(this)} \u0026gt;Button\u0026lt;/button\u0026gt; \u0026lt;button onClick={this.handleButtonClick.bind(this)} \u0026gt;Button\u0026lt;/button\u0026gt; \u0026lt;button onClick={this.handleButtonClick.bind(this)} \u0026gt;Button\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; ); }; That is not supposed to be, let us step up a bit!\nBinding in the constructor Generally, what you will find around is binding a function in the constructor, this helps to reduce the redundancy allowed by binding on the component.\nconstructor(props) { super(props); this.handleButtonClick.bind(this); } So, the component after proper binding would look like this:\nimport React, { Component } from \u0026#34;react\u0026#34;; class ButtonClickCounter extends Component { constructor(props) { super(props); this.handleButtonClick.bind(this); } state = { clickedCount: 0 }; handleButtonClick() { const clickedCount = this.state.clickedCount; console.log(\u0026#34;This Button Has Been Clicked: %s times\u0026#34;, clickedCount); this.setState({ clickedCount: clickedCount++ }); } render() { return ( \u0026lt;div\u0026gt; \u0026lt;span\u0026gt; Button clicked \u0026lt;em\u0026gt;{this.state.clickedCount}\u0026lt;/em\u0026gt; times \u0026lt;/span\u0026gt; \u0026lt;button onClick={this.handleButtonClick}\u0026gt;Button\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; ); } } Can\u0026rsquo;t we get a little bit fancier without binding at all?\nOf Course\u0026hellip; Arrow function to the rescue I love arrow functions, since its introduction through ES6 and Babel, I have seen hundreds of it, and they look so beautiful!\nThe beauty that arrow functions have to offer is that they maintain the scope of where they are declared, unlike the traditional functions that maintain the scope of where they are called.\nconst arrowFunction = () =\u0026gt; {} By simply converting handleButtonClick function to an arrow function instead, I am provided a neatly bounded function that could be used anywhere, and still can interact with my class properties as I might need.\nhandleButtonClick = () =\u0026gt; { const clickedCount = this.state.clickedCount; console.log(\u0026#34;This Button Has Been Clicked: %s times\u0026#34;, clickedCount); this.setState({ clickedCount: clickedCount++ }); } Easy, right?\nConclusion You don\u0026rsquo;t always have to keep binding every function you pass down, by writing them as arrow functions, we get a little bit of extra power of being bound to the class\u0026rsquo;s scope.\nThis does not only assist us in keeping our code clean but it also saves us from bugs due to unbounded functions. And who does not like automation?\nSwitching to writing your arrow functions is a habit I would like you to form, I have formed it, and enjoying it. It is worth it!\n","permalink":"https://limistah.dev/posts/react-class-method-binding/","summary":"Passing down functions as event handlers down to children components is a norm in the react world. It eases the communication flow, as the saying goes props down, functions up.\nThings get a little bit tricky when using a React Classical component. Functions have to maintain their scope for proper interaction with their declared class properties. You might not be lucky sometimes, so there has been a couple of workaround for this.","title":"Smart React Class function scope binding"}]